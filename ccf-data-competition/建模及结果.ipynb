{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\xgboost\\__init__.py:29: FutureWarning: Python 3.5 support is deprecated; XGBoost will require Python 3.6+ in the near future. Consider upgrading to Python 3.6+.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#数据预处理\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cab\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,precision_recall_fscore_support,roc_curve,auc,roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\generic.py:9114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:129: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\generic.py:9114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:129: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\generic.py:9114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:129: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\generic.py:9114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:129: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\generic.py:9114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:129: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#本次示例中使用了训练集共24644个数据\n",
    "with open(r'data/train_datafs.csv') as f:\n",
    "    df_new = pd.read_csv(f,parse_dates=[0],index_col=0)\n",
    "# with open(r'data/test_datafs.csv') as f:\n",
    "#     df_test = pd.read_csv(f,parse_dates=[0],index_col=0)\n",
    "#df_test = df_test.reset_index()\n",
    "#特征选择\n",
    "# select_feature=['industryco',\n",
    "#  'regcap',\n",
    "#  'reccap',\n",
    "#  'industryphy',\n",
    "#  'regcap_reccap',\n",
    "#  'TAX_AMOUNT',\n",
    "#  'enttypegb_enttypeitem',\n",
    "#  'industryphy_enttypeitem',\n",
    "#  'tfidi_opscope',\n",
    "#  'enttypegb',\n",
    "#  'gap_year',\n",
    "#  'enttypegb_industryphy',\n",
    "#  'enttypeminu',\n",
    "#  'industryphy_industryco',\n",
    "#  'enttypeitem',\n",
    "#  'townsign',\n",
    "#  'bucket_regcap',\n",
    "#  'bgxmdm',\n",
    "#  'empnum',\n",
    "#  'positive_negtive',\n",
    "#  'bucket_regcap_reccap',\n",
    "#  'exenum',\n",
    "#  'FORINVESTSIGN',\n",
    "#  'oplocdistrict',\n",
    "#  'industryco_enttypeitem',\n",
    "#  'enttypegb_enttypeitem_industryphy_industryco',\n",
    "#  'COLGRANUM',\n",
    "#  'bucket_reccap',\n",
    "#  'patent_num',\n",
    "#  'EMPNUM',\n",
    "# 'id']\n",
    "select_feature2=['industryco',\n",
    " 'regcap',\n",
    " 'reccap',\n",
    " 'industryphy',\n",
    " 'regcap_reccap',\n",
    " 'TAX_AMOUNT',\n",
    " 'enttypegb_enttypeitem',\n",
    " 'industryphy_enttypeitem',\n",
    " 'tfidi_opscope',\n",
    " 'enttypegb',\n",
    " 'gap_year',\n",
    " 'enttypegb_industryphy',\n",
    " 'enttypeminu',\n",
    " 'industryphy_industryco',\n",
    " 'enttypeitem',\n",
    " 'townsign',\n",
    " 'bucket_regcap',\n",
    " 'bgxmdm',\n",
    " 'empnum',\n",
    " 'positive_negtive',\n",
    " 'bucket_regcap_reccap',\n",
    " 'exenum',\n",
    " 'FORINVESTSIGN',\n",
    " 'oplocdistrict',\n",
    " 'industryco_enttypeitem',\n",
    " 'enttypegb_enttypeitem_industryphy_industryco',\n",
    " 'COLGRANUM',\n",
    " 'bucket_reccap',\n",
    " 'patent_num',\n",
    " 'EMPNUM',\n",
    " 'label']\n",
    "\n",
    "df_new=df_new[[i for i in df_new.columns if i in select_feature2]]\n",
    "#df_test=df_test[[i for i in df_test.columns if i in select_feature]]\n",
    "#处理'FORINVESTSIGN'\n",
    "df_1= df_new['FORINVESTSIGN'].copy()\n",
    "#df_2= df_test['FORINVESTSIGN'].copy()\n",
    "#与\n",
    "#m1 = df['FORINVESTSIGN'][(df_ != -1) & (df_ != 2)]\n",
    "\n",
    "#或\n",
    "df_new = df_new[(df_1 == -1) ^ (df_1 == 2)]\n",
    "xtrain = df_new.iloc[:,:-1]\n",
    "#ytrain = df_new.iloc[:,-1]\n",
    "#test = df_test.iloc[:,1:]\n",
    "\n",
    "#df = pd.concat([xtrain, test],axis=0)\n",
    "#df.index = range(24644)\n",
    "#分为类别特征和数值特征\n",
    "cat_features=['industryco',\n",
    " 'industryphy',\n",
    " 'regcap_reccap',\n",
    " 'enttypegb_enttypeitem',\n",
    " 'industryphy_enttypeitem',\n",
    " 'enttypegb',\n",
    " 'enttypegb_industryphy',\n",
    " 'enttypeminu',\n",
    " 'industryphy_industryco',\n",
    " 'enttypeitem',\n",
    " 'townsign',\n",
    " 'bucket_regcap',\n",
    " 'positive_negtive',\n",
    " 'bucket_regcap_reccap',\n",
    " 'FORINVESTSIGN',\n",
    " 'oplocdistrict',\n",
    " 'industryco_enttypeitem',\n",
    " 'enttypegb_enttypeitem_industryphy_industryco',\n",
    " 'bucket_reccap',\n",
    " 'patent_num']\n",
    "df_cat = df_new[[i for i in df_new.columns if i in cat_features]]\n",
    "num_feat = [ 'regcap',\n",
    "            'reccap', \n",
    "            'TAX_AMOUNT',\n",
    "            'tfidi_opscope',\n",
    "            'gap_year',\n",
    "            'bgxmdm',\n",
    "            'empnum',\n",
    "            'exenum',\n",
    "             'COLGRANUM',\n",
    "            'EMPNUM']\n",
    "df_num=df_new[[i for i in df_new.columns if i in num_feat]]\n",
    "#将每个类别特征编码改为1~len(该特征维数)，缺失值填0\n",
    "def m(df):\n",
    "    for i in cat_features:\n",
    "#    print(i)\n",
    "        labels = df[i].unique().tolist()\n",
    "        if -1 in labels:\n",
    "            labels.remove(-1)\n",
    "            q = df[i][df[i] != -1]\n",
    "            q = q.apply(lambda x:labels.index(x)+1)\n",
    "            df[i][df[i] != -1] = q\n",
    "        else:\n",
    "            df[i] = df[i].apply(lambda x:labels.index(x)+1)\n",
    "m(df_cat)\n",
    "min_cols = df_num.min()\n",
    "max_cols = df_num.max()\n",
    "df_num = (df_num - min_cols) / (max_cols - min_cols)\n",
    "df_1 = pd.concat([df_cat, df_num],axis=1)\n",
    "xtrain = df_1.iloc[:14644,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "47645761dc56bb8c5fae00114b768b5d9b6e917c3aec07c4    0\n",
       "59b38c56de3836838082cfcb1a298951abfe15e6940c49ba    0\n",
       "e9f7b28ec10e047000d16ab79e1b5e6da434a1697cce7818    0\n",
       "f000950527a6feb63ee1ce82bb22ddd1ab8b8fdffa3b91fb    0\n",
       "9c7fa510616a6830b878f3c8c4317d93e1b022e7f22ae231    0\n",
       "                                                   ..\n",
       "59b38c56de3836837421cc5322074e10bb13a74257eb1969    1\n",
       "f000950527a6feb6a532f0b773f795bb428e8646e87448e7    0\n",
       "59b38c56de3836835d73a7fa9a782082ade3433adf3aaebc    0\n",
       "f000950527a6feb62bf5fe41b57d6b3f599e32dd8c68f172    0\n",
       "216bd2aaf4d07924009c6a8241b306e5a6050602204ebbbf    0\n",
       "Name: label, Length: 14644, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain = df_new.iloc[:,-1]\n",
    "ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "dfTrain, dfTest, ytrain, ytest = train_test_split(xtrain,ytrain,test_size=0.3)#0.3是常用的划分,随机划分后数据的索引乱了\n",
    "for i in [dfTrain, dfTest, ytrain, ytest]:\n",
    "    i.index = range(i.shape[0])#索引换为0-shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#deepfm网络处理缺失值的方法\n",
    "def preprocess(df):\n",
    "    cols = list(df.columns)#[c for c in df.columns if c not in [\"id\", \"target\"]]\n",
    "    df[\"missing_feat\"] = np.sum((df[cols] == -1).values, axis=1)\n",
    "    return df\n",
    "dfTrain = preprocess(dfTrain)\n",
    "dfTest = preprocess(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([dfTrain,dfTest],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = range(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLS = num_feat\n",
    "IGNORE_COLS = [\n",
    "     \"label\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3486\n",
      "{'EMPNUM': 3476, 'bucket_reccap': {1: 1117, 2: 1118, 3: 1119}, 'enttypegb_enttypeitem_industryphy_industryco': {1: 2658, 2: 2768, 3: 2933, 4: 2577, 5: 2576, 6: 2721, 7: 2556, 8: 2565, 9: 2553, 10: 2531, 11: 2750, 12: 2799, 13: 2523, 14: 2614, 15: 2788, 16: 2528, 17: 2584, 18: 2978, 19: 2701, 20: 2542, 21: 2769, 22: 2555, 23: 2529, 24: 2626, 25: 2641, 26: 2535, 27: 2558, 28: 2994, 29: 2650, 30: 2591, 31: 2549, 32: 2599, 33: 2603, 34: 2539, 35: 2550, 36: 2589, 37: 2612, 38: 2536, 39: 3279, 40: 2613, 41: 2643, 42: 2537, 43: 2547, 44: 3060, 45: 3013, 46: 2684, 47: 3311, 48: 2582, 49: 2849, 50: 2600, 51: 2530, 52: 2947, 53: 2826, 54: 3265, 55: 2836, 56: 2822, 57: 2532, 58: 2861, 59: 2573, 60: 2860, 61: 2557, 62: 2534, 63: 2651, 64: 2562, 65: 2635, 66: 3141, 67: 2832, 68: 2545, 69: 2581, 70: 2611, 71: 2853, 72: 2571, 73: 2691, 74: 2588, 75: 3161, 76: 2708, 77: 2740, 78: 2566, 79: 2692, 80: 3074, 81: 2617, 82: 2649, 83: 2752, 84: 2569, 85: 2618, 86: 2655, 87: 2593, 88: 2559, 89: 3348, 90: 2732, 91: 2980, 92: 2891, 93: 2783, 94: 2678, 95: 2541, 96: 2625, 97: 2934, 98: 2744, 99: 2592, 100: 2663, 101: 2639, 102: 2533, 103: 2700, 104: 2578, 105: 2802, 106: 2609, 107: 3007, 108: 2527, 109: 2733, 110: 2590, 111: 2563, 112: 2628, 113: 2857, 114: 2598, 115: 2690, 116: 3215, 117: 2770, 118: 2839, 119: 3094, 120: 2587, 121: 3066, 122: 2616, 123: 2937, 124: 3047, 125: 2522, 126: 2843, 127: 2998, 128: 2645, 129: 2564, 130: 2608, 131: 2868, 132: 2956, 133: 2791, 134: 2551, 135: 2544, 136: 2634, 137: 2771, 138: 2830, 139: 2970, 140: 2707, 141: 2594, 142: 3183, 143: 2568, 144: 2653, 145: 2524, 146: 2714, 147: 2743, 148: 2687, 149: 2716, 150: 2681, 151: 3000, 152: 2790, 153: 2677, 154: 2644, 155: 3345, 156: 2739, 157: 2586, 158: 2722, 159: 2680, 160: 3005, 161: 2796, 162: 2899, 163: 2795, 164: 2794, 165: 2548, 166: 2705, 167: 2895, 168: 3149, 169: 2815, 170: 2730, 171: 2574, 172: 2862, 173: 2981, 174: 2737, 175: 3174, 176: 2543, 177: 2604, 178: 3256, 179: 2696, 180: 2709, 181: 2875, 182: 2995, 183: 2624, 184: 2878, 185: 2902, 186: 3040, 187: 2671, 188: 3334, 189: 3090, 190: 2724, 191: 2640, 192: 2926, 193: 3054, 194: 2665, 195: 2751, 196: 2828, 197: 2779, 198: 2723, 199: 2660, 200: 3379, 201: 3322, 202: 2688, 203: 2679, 204: 2647, 205: 2766, 206: 2808, 207: 2637, 208: 2685, 209: 2834, 210: 2654, 211: 2607, 212: 2818, 213: 2699, 214: 2620, 215: 3207, 216: 2990, 217: 2792, 218: 2819, 219: 2712, 220: 2602, 221: 2657, 222: 2682, 223: 3049, 224: 2755, 225: 3017, 226: 2803, 227: 3176, 228: 2567, 229: 3038, 230: 3212, 231: 3055, 232: 2765, 233: 2972, 234: 2873, 235: 2778, 236: 2605, 237: 2938, 238: 2872, 239: 2652, 240: 3341, 241: 2638, 242: 2554, 243: 2673, 244: 2833, 245: 2888, 246: 3208, 247: 2949, 248: 2928, 249: 3002, 250: 3175, 251: 2977, 252: 2572, 253: 2597, 254: 3162, 255: 2686, 256: 3331, 257: 2920, 258: 2719, 259: 2675, 260: 2838, 261: 2984, 262: 3140, 263: 3051, 264: 3357, 265: 2735, 266: 2809, 267: 2780, 268: 2546, 269: 3187, 270: 3238, 271: 2820, 272: 2786, 273: 3062, 274: 3006, 275: 2734, 276: 2629, 277: 2854, 278: 3338, 279: 2718, 280: 2601, 281: 2806, 282: 2526, 283: 3045, 284: 3056, 285: 2648, 286: 2754, 287: 2760, 288: 3106, 289: 2674, 290: 2825, 291: 2958, 292: 2801, 293: 3095, 294: 3362, 295: 3343, 296: 3188, 297: 2890, 298: 2852, 299: 2927, 300: 3134, 301: 2636, 302: 2683, 303: 3064, 304: 2729, 305: 2694, 306: 2879, 307: 2941, 308: 2621, 309: 3281, 310: 3274, 311: 3213, 312: 2693, 313: 2804, 314: 2774, 315: 3092, 316: 3171, 317: 3222, 318: 3115, 319: 3145, 320: 3009, 321: 2560, 322: 2835, 323: 3254, 324: 3071, 325: 2738, 326: 2993, 327: 2741, 328: 3015, 329: 3180, 330: 2975, 331: 2579, 332: 3113, 333: 3124, 334: 2782, 335: 3144, 336: 2950, 337: 2727, 338: 3050, 339: 2965, 340: 2954, 341: 2889, 342: 3088, 343: 3029, 344: 2922, 345: 2866, 346: 2757, 347: 2886, 348: 2915, 349: 2914, 350: 3381, 351: 2695, 352: 2670, 353: 2871, 354: 2725, 355: 3223, 356: 3133, 357: 3234, 358: 3173, 359: 2811, 360: 2668, 361: 2837, 362: 3137, 363: 3032, 364: 2812, 365: 2881, 366: 3018, 367: 3298, 368: 3309, 369: 2876, 370: 3086, 371: 2615, 372: 2762, 373: 3296, 374: 3142, 375: 3034, 376: 2894, 377: 2960, 378: 3302, 379: 3369, 380: 2932, 381: 3356, 382: 3083, 383: 3249, 384: 3027, 385: 2538, 386: 2883, 387: 3070, 388: 2831, 389: 3100, 390: 2952, 391: 2897, 392: 3084, 393: 2962, 394: 3260, 395: 2845, 396: 3150, 397: 3315, 398: 3109, 399: 3159, 400: 2672, 401: 2903, 402: 2583, 403: 3011, 404: 3059, 405: 2814, 406: 2874, 407: 2884, 408: 2759, 409: 3264, 410: 3097, 411: 3382, 412: 2521, 413: 3193, 414: 3033, 415: 2642, 416: 3363, 417: 2943, 418: 2896, 419: 2918, 420: 2800, 421: 2824, 422: 2939, 423: 2898, 424: 3111, 425: 2991, 426: 2940, 427: 2763, 428: 2817, 429: 3151, 430: 3282, 431: 3275, 432: 3233, 433: 3065, 434: 3247, 435: 3358, 436: 3120, 437: 2610, 438: 3099, 439: 2646, 440: 3372, 441: 3126, 442: 2697, 443: 2731, 444: 3125, 445: 3308, 446: 2966, 447: 3248, 448: 3269, 449: 2781, 450: 3347, 451: 3389, 452: 3387, 453: 3042, 454: 2710, 455: 3127, 456: 2864, 457: 2968, 458: 2704, 459: 2676, 460: 2713, 461: 2987, 462: 3340, 463: 3080, 464: 2784, 465: 2813, 466: 2666, 467: 2619, 468: 2974, 469: 3371, 470: 3073, 471: 2630, 472: 2942, 473: 2746, 474: 3268, 475: 3135, 476: 3102, 477: 3198, 478: 2997, 479: 2983, 480: 3046, 481: 3025, 482: 2870, 483: 2793, 484: 2758, 485: 2623, 486: 3258, 487: 3041, 488: 3068, 489: 2964, 490: 2992, 491: 3305, 492: 2715, 493: 2924, 494: 2596, 495: 2595, 496: 3118, 497: 2775, 498: 3098, 499: 2816, 500: 2656, 501: 2669, 502: 3300, 503: 3037, 504: 3057, 505: 3329, 506: 3170, 507: 2662, 508: 3297, 509: 3396, 510: 2892, 511: 2893, 512: 2631, 513: 2865, 514: 3394, 515: 2747, 516: 2848, 517: 2745, 518: 3270, 519: 2908, 520: 3272, 521: 3245, 522: 2921, 523: 2885, 524: 2859, 525: 3044, 526: 3393, 527: 3199, 528: 3016, 529: 2570, 530: 3104, 531: 3235, 532: 3036, 533: 2967, 534: 2726, 535: 3378, 536: 3077, 537: 2863, 538: 2880, 539: 3283, 540: 2869, 541: 3172, 542: 3035, 543: 3280, 544: 3078, 545: 3021, 546: 3004, 547: 3337, 548: 3075, 549: 3107, 550: 2807, 551: 3353, 552: 2689, 553: 3139, 554: 3220, 555: 3129, 556: 2764, 557: 3239, 558: 3271, 559: 3346, 560: 3053, 561: 3237, 562: 2841, 563: 3048, 564: 3085, 565: 2989, 566: 3012, 567: 2925, 568: 3217, 569: 2661, 570: 2904, 571: 3377, 572: 3242, 573: 3110, 574: 3087, 575: 3288, 576: 3168, 577: 2633, 578: 2720, 579: 3039, 580: 3204, 581: 3081, 582: 3061, 583: 3158, 584: 3143, 585: 3019, 586: 3186, 587: 3165, 588: 2664, 589: 3069, 590: 3333, 591: 3365, 592: 3205, 593: 3218, 594: 3263, 595: 2951, 596: 2973, 597: 3313, 598: 2936, 599: 3184, 600: 2986, 601: 3160, 602: 2945, 603: 3163, 604: 3368, 605: 3147, 606: 3020, 607: 2756, 608: 3022, 609: 3262, 610: 3314, 611: 3257, 612: 3321, 613: 2580, 614: 2969, 615: 3364, 616: 3001, 617: 2930, 618: 2827, 619: 3181, 620: 2698, 621: 2525, 622: 3293, 623: 3307, 624: 3228, 625: 2717, 626: 3316, 627: 2753, 628: 3240, 629: 2913, 630: 3043, 631: 3131, 632: 3003, 633: 2946, 634: 2855, 635: 2552, 636: 3195, 637: 3093, 638: 2772, 639: 2798, 640: 2988, 641: 3211, 642: 3367, 643: 3395, 644: 3253, 645: 2856, 646: 2919, 647: 3384, 648: 2736, 649: 3058, 650: 3117, 651: 2923, 652: 3067, 653: 3103, 654: 3284, 655: 3121, 656: 2632, 657: 3349, 658: 2585, 659: 3236, 660: 3230, 661: 3278, 662: 3196, 663: 3177, 664: 3359, 665: 3219, 666: 3028, 667: 3339, 668: 2703, 669: 3352, 670: 3332, 671: 2955, 672: 3216, 673: 3154, 674: 2797, 675: 3209, 676: 3105, 677: 3301, 678: 3328, 679: 2882, 680: 2829, 681: 2761, 682: 3192, 683: 3024, 684: 3214, 685: 2821, 686: 2540, 687: 3079, 688: 3191, 689: 3324, 690: 3026, 691: 2776, 692: 2749, 693: 3157, 694: 3152, 695: 3031, 696: 3318, 697: 3259, 698: 3287, 699: 3374, 700: 3179, 701: 3155, 702: 3226, 703: 2982, 704: 3360, 705: 3261, 706: 2810, 707: 3294, 708: 3148, 709: 3185, 710: 3072, 711: 3156, 712: 3392, 713: 3190, 714: 3108, 715: 2846, 716: 2935, 717: 3153, 718: 3008, 719: 2912, 720: 3244, 721: 3116, 722: 3251, 723: 3397, 724: 3370, 725: 3201, 726: 3241, 727: 2858, 728: 2659, 729: 3290, 730: 3342, 731: 2805, 732: 3136, 733: 3385, 734: 3277, 735: 3351, 736: 3390, 737: 3354, 738: 3063, 739: 2728, 740: 3231, 741: 3330, 742: 3266, 743: 3383, 744: 3010, 745: 2777, 746: 2999, 747: 3023, 748: 3320, 749: 2622, 750: 3366, 751: 3146, 752: 3373, 753: 2911, 754: 3312, 755: 3289, 756: 3052, 757: 2606, 758: 3166, 759: 3250, 760: 2901, 761: 2963, 762: 3229, 763: 2996, 764: 2702, 765: 3355, 766: 3327, 767: 3128, 768: 2867, 769: 3197, 770: 3224, 771: 2917, 772: 3200, 773: 2823, 774: 3096, 775: 2877, 776: 3335, 777: 2944, 778: 3167, 779: 2850, 780: 3273, 781: 3221, 782: 2957, 783: 2787, 784: 3132, 785: 3082, 786: 2785, 787: 2900, 788: 2905, 789: 3323, 790: 3299, 791: 2976, 792: 2959, 793: 2985, 794: 3225, 795: 3014, 796: 3112, 797: 2842, 798: 3232, 799: 2851, 800: 3255, 801: 2948, 802: 2742, 803: 2748, 804: 3091, 805: 3310, 806: 3227, 807: 2906, 808: 2910, 809: 2916, 810: 3267, 811: 3122, 812: 2847, 813: 3169, 814: 3114, 815: 3182, 816: 3303, 817: 3210, 818: 3130, 819: 3203, 820: 3101, 821: 2907, 822: 2667, 823: 2929, 824: 3319, 825: 3295, 826: 2931, 827: 3189, 828: 2627, 829: 3194, 830: 2887, 831: 3030, 832: 2953, 833: 2909, 834: 2844, 835: 3336, 836: 3164, 837: 2706, 838: 3291, 839: 3380, 840: 2961, 841: 3306, 842: 3375, 843: 3361, 844: 3206, 845: 3202, 846: 2575, 847: 3292, 848: 3376, 849: 2561, 850: 3089, 851: 3325, 852: 3386, 853: 3178, 854: 2979, 855: 3252, 856: 3286, 857: 3350, 858: 3317, 859: 2773, 860: 3344, 861: 3138, 862: 3123, 863: 3388, 864: 3326, 865: 2971, 866: 3391, 867: 3246, 868: 3304, 869: 2767, 870: 3076, 871: 3285, 872: 3276, 873: 3119, 874: 3243, 875: 2711, 876: 2789, 877: 2840}, 'enttypeitem': {1: 311, 2: 307, 3: 308, 4: 310, 5: 314, 6: 309, 7: 315, 8: 320, 9: 321, 10: 325, 11: 328, 12: 322, 13: 331, 14: 319, 15: 317, 16: 324, 17: 330, 18: 318, 19: 333, 20: 316, 21: 334, 22: 327, 23: 335, 24: 323, 25: 313, 26: 332, 27: 329, 28: 326, -1: 312}, 'enttypegb_enttypeitem': {1: 1133, 2: 1129, 3: 1130, 4: 1134, 5: 1132, 6: 1140, 7: 1136, 8: 1131, 9: 1137, 10: 1138, 11: 1152, 12: 1146, 13: 1148, 14: 1154, 15: 1143, 16: 1161, 17: 1160, 18: 1147, 19: 1150, 20: 1165, 21: 1144, 22: 1162, 23: 1155, 24: 1164, 25: 1142, 26: 1170, 27: 1171, 28: 1149, 29: 1139, 30: 1172, 31: 1158, 32: 1168, 33: 1174, 34: 1141, 35: 1153, 36: 1166, 37: 1151, 38: 1145, 39: 1159, 40: 1135, 41: 1156, 42: 1173, 43: 1167, 44: 1169, 45: 1163, 46: 1157}, 'positive_negtive': {1: 3402, 2: 3401, 3: 3403, -1: 3400}, 'industryphy_enttypeitem': {1: 1642, 2: 1641, 3: 1717, 4: 1653, 5: 1643, 6: 1640, 7: 1638, 8: 1652, 9: 1647, 10: 1699, 11: 1639, 12: 1648, 13: 1662, 14: 1667, 15: 1656, 16: 1650, 17: 1644, 18: 1645, 19: 1658, 20: 1763, 21: 1702, 22: 1668, 23: 1676, 24: 1649, 25: 1655, 26: 1669, 27: 1660, 28: 1665, 29: 1678, 30: 1679, 31: 1688, 32: 1691, 33: 1673, 34: 1674, 35: 1686, 36: 1637, 37: 1771, 38: 1729, 39: 1698, 40: 1661, 41: 1730, 42: 1681, 43: 1664, 44: 1711, 45: 1739, 46: 1675, 47: 1671, 48: 1710, 49: 1703, 50: 1720, 51: 1685, 52: 1728, 53: 1769, 54: 1715, 55: 1704, 56: 1752, 57: 1696, 58: 1742, 59: 1697, 60: 1677, 61: 1725, 62: 1680, 63: 1740, 64: 1748, 65: 1707, 66: 1709, 67: 1736, 68: 1672, 69: 1713, 70: 1654, 71: 1754, 72: 1758, 73: 1701, 74: 1690, 75: 1759, 76: 1684, 77: 1689, 78: 1724, 79: 1646, 80: 1721, 81: 1746, 82: 1700, 83: 1737, 84: 1775, 85: 1694, 86: 1683, 87: 1666, 88: 1770, 89: 1695, 90: 1670, 91: 1663, 92: 1692, 93: 1706, 94: 1760, 95: 1657, 96: 1726, 97: 1708, 98: 1682, 99: 1744, 100: 1747, 101: 1772, 102: 1733, 103: 1751, 104: 1738, 105: 1755, 106: 1718, 107: 1731, 108: 1750, 109: 1766, 110: 1723, 111: 1651, 112: 1732, 113: 1659, 114: 1774, 115: 1757, 116: 1745, 117: 1753, 118: 1768, 119: 1762, 120: 1764, 121: 1743, 122: 1716, 123: 1749, 124: 1693, 125: 1741, 126: 1734, 127: 1727, 128: 1735, 129: 1705, 130: 1719, 131: 1687, 132: 1767, 133: 1712, 134: 1714, 135: 1765, 136: 1722, 137: 1756, 138: 1761, 139: 1773}, 'empnum': 3471, 'patent_num': {1: 3462, 2: 3411, 3: 3468, 4: 3420, 5: 3460, 6: 3418, 7: 3405, 8: 3429, 9: 3417, 10: 3447, 11: 3461, 12: 3456, 13: 3406, 14: 3410, 15: 3437, 16: 3452, 17: 3415, 18: 3431, 19: 3408, 20: 3428, 21: 3434, 22: 3421, 23: 3414, 24: 3409, 25: 3425, 26: 3453, 27: 3435, 28: 3444, 29: 3426, 30: 3423, 31: 3422, 32: 3419, 33: 3459, 34: 3440, 35: 3407, 36: 3443, 37: 3433, 38: 3438, 39: 3466, 40: 3445, 41: 3439, 42: 3432, 43: 3413, 44: 3424, 45: 3427, 46: 3436, 47: 3416, 48: 3469, 49: 3458, 50: 3451, 51: 3448, 52: 3455, 53: 3449, 54: 3463, 55: 3446, 56: 3450, 57: 3441, 58: 3464, 59: 3412, 60: 3457, 61: 3467, 62: 3470, 63: 3430, 64: 3442, 65: 3454, 66: 3465, -1: 3404}, 'bucket_regcap_reccap': {1: 1123, 2: 1127, 3: 1125, 4: 1124, 5: 1120, 6: 1121, 7: 1122, 8: 1128, 9: 1126}, 'reccap': 3474, 'oplocdistrict': {1: 8, 2: 2, 3: 1, 4: 7, 5: 5, 6: 9, 7: 0, 8: 4, 9: 3, 10: 6, 11: 11, 12: 10, 13: 12, 14: 13}, 'missing_feat': {0: 3485, 1: 3484, 2: 3483, 3: 3481, 4: 3480, 5: 3482}, 'enttypegb_industryphy': {1: 1454, 2: 1453, 3: 1552, 4: 1465, 5: 1455, 6: 1452, 7: 1450, 8: 1464, 9: 1495, 10: 1524, 11: 1451, 12: 1460, 13: 1474, 14: 1481, 15: 1459, 16: 1477, 17: 1512, 18: 1462, 19: 1456, 20: 1457, 21: 1499, 22: 1566, 23: 1482, 24: 1470, 25: 1621, 26: 1528, 27: 1483, 28: 1491, 29: 1461, 30: 1467, 31: 1484, 32: 1472, 33: 1476, 34: 1479, 35: 1496, 36: 1497, 37: 1507, 38: 1515, 39: 1511, 40: 1488, 41: 1522, 42: 1489, 43: 1492, 44: 1575, 45: 1505, 46: 1449, 47: 1629, 48: 1569, 49: 1523, 50: 1473, 51: 1570, 52: 1498, 53: 1510, 54: 1478, 55: 1538, 56: 1468, 57: 1585, 58: 1490, 59: 1486, 60: 1537, 61: 1529, 62: 1540, 63: 1557, 64: 1576, 65: 1568, 66: 1627, 67: 1548, 68: 1530, 69: 1554, 70: 1603, 71: 1520, 72: 1588, 73: 1521, 74: 1550, 75: 1562, 76: 1555, 77: 1596, 78: 1598, 79: 1578, 80: 1539, 81: 1581, 82: 1487, 83: 1579, 84: 1549, 85: 1494, 86: 1466, 87: 1613, 88: 1614, 89: 1527, 90: 1509, 91: 1615, 92: 1503, 93: 1508, 94: 1561, 95: 1458, 96: 1617, 97: 1558, 98: 1620, 99: 1526, 100: 1582, 101: 1607, 102: 1633, 103: 1517, 104: 1525, 105: 1546, 106: 1634, 107: 1502, 108: 1480, 109: 1628, 110: 1518, 111: 1485, 112: 1475, 113: 1536, 114: 1595, 115: 1514, 116: 1493, 117: 1589, 118: 1533, 119: 1616, 120: 1534, 121: 1513, 122: 1469, 123: 1563, 124: 1545, 125: 1535, 126: 1519, 127: 1500, 128: 1591, 129: 1597, 130: 1630, 131: 1586, 132: 1573, 133: 1601, 134: 1602, 135: 1504, 136: 1584, 137: 1610, 138: 1565, 139: 1553, 140: 1571, 141: 1600, 142: 1564, 143: 1624, 144: 1560, 145: 1532, 146: 1463, 147: 1572, 148: 1547, 149: 1592, 150: 1471, 151: 1608, 152: 1632, 153: 1612, 154: 1593, 155: 1606, 156: 1626, 157: 1619, 158: 1635, 159: 1594, 160: 1590, 161: 1551, 162: 1605, 163: 1599, 164: 1577, 165: 1636, 166: 1622, 167: 1516, 168: 1583, 169: 1587, 170: 1541, 171: 1574, 172: 1501, 173: 1609, 174: 1567, 175: 1580, 176: 1531, 177: 1556, 178: 1506, 179: 1625, 180: 1542, 181: 1544, 182: 1623, 183: 1543, 184: 1604, 185: 1559, 186: 1611, 187: 1618, 188: 1631}, 'COLGRANUM': 3477, 'townsign': {1: 336, 2: 337}, 'industryco_enttypeitem': {1: 1874, 2: 2005, 3: 2154, 4: 1832, 5: 1831, 6: 1964, 7: 1811, 8: 1820, 9: 1808, 10: 1786, 11: 1794, 12: 2031, 13: 1778, 14: 1867, 15: 2021, 16: 1783, 17: 1839, 18: 2162, 19: 1946, 20: 1797, 21: 1785, 22: 1810, 23: 1784, 24: 1878, 25: 1892, 26: 1790, 27: 1813, 28: 2209, 29: 1901, 30: 1845, 31: 1804, 32: 1853, 33: 1857, 34: 1805, 35: 1843, 36: 1841, 37: 1791, 38: 2430, 39: 1866, 40: 1894, 41: 1792, 42: 1802, 43: 2216, 44: 1930, 45: 2024, 46: 1837, 47: 2074, 48: 1854, 49: 2168, 50: 2053, 51: 2420, 52: 2061, 53: 2049, 54: 1787, 55: 2086, 56: 1828, 57: 2085, 58: 1812, 59: 1789, 60: 1902, 61: 1817, 62: 1887, 63: 2324, 64: 2058, 65: 1800, 66: 1836, 67: 1865, 68: 2078, 69: 1826, 70: 1936, 71: 1842, 72: 2342, 73: 1953, 74: 1982, 75: 1821, 76: 1937, 77: 1864, 78: 1870, 79: 1900, 80: 1992, 81: 1824, 82: 1871, 83: 1906, 84: 1847, 85: 1814, 86: 2482, 87: 1974, 88: 1921, 89: 2115, 90: 2016, 91: 1925, 92: 1796, 93: 1877, 94: 2155, 95: 1985, 96: 1846, 97: 1913, 98: 1890, 99: 1788, 100: 1945, 101: 1833, 102: 2033, 103: 1863, 104: 2219, 105: 1782, 106: 1975, 107: 1844, 108: 1818, 109: 1880, 110: 2082, 111: 1852, 112: 2382, 113: 2006, 114: 2064, 115: 2284, 116: 2268, 117: 1869, 118: 2158, 119: 1777, 120: 2068, 121: 2212, 122: 1896, 123: 1819, 124: 1862, 125: 2093, 126: 2177, 127: 1806, 128: 1799, 129: 1886, 130: 2007, 131: 1950, 132: 2075, 133: 1952, 134: 1848, 135: 2119, 136: 1823, 137: 1904, 138: 1779, 139: 1822, 140: 1933, 141: 1960, 142: 1928, 143: 2023, 144: 1924, 145: 1895, 146: 2479, 147: 1981, 148: 1927, 149: 2218, 150: 2028, 151: 2123, 152: 2027, 153: 2026, 154: 1803, 155: 2332, 156: 2044, 157: 1972, 158: 1829, 159: 2087, 160: 2196, 161: 1979, 162: 2333, 163: 1798, 164: 1858, 165: 2413, 166: 1941, 167: 1954, 168: 2100, 169: 1868, 170: 1876, 171: 2103, 172: 2126, 173: 2248, 174: 2471, 175: 1922, 176: 1966, 177: 1891, 178: 2147, 179: 1934, 180: 1915, 181: 1991, 182: 2055, 183: 2012, 184: 1965, 185: 1910, 186: 2506, 187: 2405, 188: 1926, 189: 1898, 190: 2037, 191: 1931, 192: 1905, 193: 1861, 194: 2046, 195: 1944, 196: 1873, 197: 2378, 198: 2205, 199: 2047, 200: 1957, 201: 1856, 202: 1908, 203: 2255, 204: 1995, 205: 2227, 206: 2034, 207: 2353, 208: 2380, 209: 2259, 210: 2004, 211: 2098, 212: 2011, 213: 1859, 214: 2159, 215: 2097, 216: 1903, 217: 2476, 218: 1889, 219: 1809, 220: 2059, 221: 2113, 222: 2170, 223: 2149, 224: 2215, 225: 2352, 226: 2194, 227: 1827, 228: 1851, 229: 2306, 230: 1932, 231: 2469, 232: 2142, 233: 1962, 234: 2063, 235: 2199, 236: 2323, 237: 2257, 238: 1977, 239: 2038, 240: 2013, 241: 1801, 242: 2359, 243: 2048, 244: 2019, 245: 2265, 246: 1976, 247: 1881, 248: 2079, 249: 1961, 250: 1855, 251: 1781, 252: 2252, 253: 2260, 254: 1899, 255: 1994, 256: 2000, 257: 2295, 258: 2052, 259: 2179, 260: 2032, 261: 2285, 262: 1923, 263: 2477, 264: 2360, 265: 2114, 266: 2077, 267: 2148, 268: 2318, 269: 1888, 270: 1929, 271: 2267, 272: 1971, 273: 1939, 274: 2104, 275: 2431, 276: 2381, 277: 1938, 278: 2351, 279: 2388, 280: 2302, 281: 2328, 282: 2220, 283: 1815, 284: 2060, 285: 1919, 286: 2190, 287: 1980, 288: 2208, 289: 1983, 290: 2225, 291: 2192, 292: 1834, 293: 2300, 294: 2309, 295: 2015, 296: 2327, 297: 2171, 298: 1969, 299: 2256, 300: 2184, 301: 2175, 302: 2280, 303: 2239, 304: 2091, 305: 1997, 306: 2111, 307: 2137, 308: 2507, 309: 1940, 310: 2096, 311: 1967, 312: 2365, 313: 2317, 314: 2399, 315: 2040, 316: 1918, 317: 2062, 318: 2321, 319: 2241, 320: 2041, 321: 2106, 322: 2228, 323: 2453, 324: 2101, 325: 2279, 326: 2002, 327: 2443, 328: 2325, 329: 2243, 330: 2118, 331: 2180, 332: 2446, 333: 2500, 334: 2153, 335: 2490, 336: 2277, 337: 2409, 338: 2237, 339: 1793, 340: 2108, 341: 2270, 342: 2057, 343: 2290, 344: 2173, 345: 2121, 346: 1993, 347: 2182, 348: 2305, 349: 2070, 350: 2458, 351: 2297, 352: 2340, 353: 1920, 354: 2127, 355: 1838, 356: 2222, 357: 2263, 358: 2043, 359: 2099, 360: 2109, 361: 1999, 362: 2419, 363: 2287, 364: 2508, 365: 1776, 366: 2242, 367: 1893, 368: 2495, 369: 2164, 370: 2120, 371: 2140, 372: 1948, 373: 2051, 374: 2160, 375: 2122, 376: 2293, 377: 2206, 378: 2161, 379: 2045, 380: 2334, 381: 2432, 382: 2427, 383: 2398, 384: 2229, 385: 2491, 386: 2307, 387: 2289, 388: 1897, 389: 2311, 390: 1942, 391: 1973, 392: 2310, 393: 2452, 394: 2185, 395: 2408, 396: 2014, 397: 2481, 398: 2513, 399: 2512, 400: 2250, 401: 1955, 402: 2312, 403: 2089, 404: 2187, 405: 1949, 406: 1958, 407: 2202, 408: 2475, 409: 2276, 410: 2017, 411: 2042, 412: 1916, 413: 1872, 414: 2191, 415: 2502, 416: 2272, 417: 1882, 418: 2163, 419: 1987, 420: 2423, 421: 2319, 422: 2292, 423: 2369, 424: 2211, 425: 2198, 426: 2253, 427: 2235, 428: 2095, 429: 2025, 430: 1998, 431: 1875, 432: 2415, 433: 2249, 434: 2183, 435: 2207, 436: 2449, 437: 1959, 438: 2145, 439: 1850, 440: 1849, 441: 2009, 442: 2288, 443: 1907, 444: 2246, 445: 2261, 446: 2467, 447: 2350, 448: 1912, 449: 2444, 450: 2519, 451: 2116, 452: 2117, 453: 1883, 454: 2090, 455: 2517, 456: 1988, 457: 2073, 458: 1986, 459: 2131, 460: 2425, 461: 2407, 462: 2143, 463: 2110, 464: 2084, 465: 2370, 466: 2226, 467: 1825, 468: 2176, 469: 2245, 470: 2186, 471: 1968, 472: 2274, 473: 2088, 474: 2105, 475: 2433, 476: 2094, 477: 2244, 478: 2275, 479: 2231, 480: 2217, 481: 2473, 482: 2273, 483: 2036, 484: 2487, 485: 1935, 486: 2386, 487: 2314, 488: 2003, 489: 1963, 490: 2424, 491: 2480, 492: 2401, 493: 2066, 494: 2254, 495: 2278, 496: 2204, 497: 2223, 498: 2146, 499: 1911, 500: 2404, 501: 2298, 502: 2329, 503: 2348, 504: 1885, 505: 2247, 506: 2375, 507: 2264, 508: 2326, 509: 2358, 510: 2345, 511: 1914, 512: 2470, 513: 2497, 514: 2376, 515: 2384, 516: 2418, 517: 2172, 518: 2456, 519: 2157, 520: 2201, 521: 2341, 522: 2166, 523: 2343, 524: 2499, 525: 2330, 526: 2230, 527: 1996, 528: 2232, 529: 2417, 530: 2457, 531: 2414, 532: 1835, 533: 2188, 534: 2496, 535: 2214, 536: 2151, 537: 2054, 538: 1943, 539: 1780, 540: 2441, 541: 2451, 542: 2393, 543: 2459, 544: 2402, 545: 2136, 546: 2251, 547: 2315, 548: 2167, 549: 2080, 550: 1807, 551: 2283, 552: 2030, 553: 2203, 554: 1879, 555: 2518, 556: 2081, 557: 2141, 558: 1978, 559: 2262, 560: 2304, 561: 2144, 562: 2269, 563: 2434, 564: 2308, 565: 1884, 566: 2483, 567: 1840, 568: 2400, 569: 2395, 570: 2429, 571: 2367, 572: 2354, 573: 2492, 574: 2385, 575: 2238, 576: 2474, 577: 2486, 578: 2322, 579: 2383, 580: 2337, 581: 2029, 582: 2379, 583: 2294, 584: 2445, 585: 2316, 586: 2107, 587: 2056, 588: 2001, 589: 2364, 590: 2234, 591: 1795, 592: 2363, 593: 2464, 594: 2236, 595: 1990, 596: 2335, 597: 2416, 598: 2436, 599: 2504, 600: 2020, 601: 2338, 602: 2391, 603: 2197, 604: 2493, 605: 2039, 606: 2331, 607: 2357, 608: 2271, 609: 2339, 610: 2516, 611: 2362, 612: 2296, 613: 2071, 614: 2156, 615: 2336, 616: 2135, 617: 2406, 618: 2303, 619: 2520, 620: 2501, 621: 2372, 622: 2403, 623: 2083, 624: 1909, 625: 2438, 626: 2035, 627: 2320, 628: 2510, 629: 2428, 630: 2485, 631: 2514, 632: 2488, 633: 2266, 634: 1970, 635: 2396, 636: 2468, 637: 2421, 638: 2509, 639: 2221, 640: 2010, 641: 2213, 642: 2233, 643: 2462, 644: 2498, 645: 2503, 646: 2134, 647: 2455, 648: 2437, 649: 2258, 650: 1860, 651: 2346, 652: 2410, 653: 2125, 654: 2394, 655: 2210, 656: 1947, 657: 2489, 658: 2313, 659: 2092, 660: 2368, 661: 2389, 662: 2139, 663: 2371, 664: 2050, 665: 2286, 666: 2102, 667: 2165, 668: 2347, 669: 2426, 670: 2387, 671: 2178, 672: 2018, 673: 2124, 674: 2128, 675: 2463, 676: 2193, 677: 2200, 678: 2390, 679: 2224, 680: 2299, 681: 2067, 682: 2397, 683: 2076, 684: 2412, 685: 2169, 686: 1984, 687: 1989, 688: 2282, 689: 2454, 690: 2392, 691: 2129, 692: 2133, 693: 2138, 694: 2422, 695: 2072, 696: 2349, 697: 2301, 698: 2356, 699: 2447, 700: 2374, 701: 2291, 702: 2130, 703: 1917, 704: 2150, 705: 2461, 706: 2442, 707: 2152, 708: 2361, 709: 2366, 710: 2112, 711: 2240, 712: 2174, 713: 2132, 714: 2069, 715: 2472, 716: 2344, 717: 1951, 718: 2439, 719: 2181, 720: 2450, 721: 2494, 722: 2377, 723: 2373, 724: 1830, 725: 2440, 726: 2505, 727: 1816, 728: 2281, 729: 2465, 730: 2511, 731: 2355, 732: 2195, 733: 2411, 734: 2435, 735: 2484, 736: 2460, 737: 2008, 738: 2478, 739: 2466, 740: 2189, 741: 2515, 742: 2448, 743: 1956, 744: 2022, 745: 2065}, 'bucket_regcap': {1: 1112, 2: 1115, 3: 1114, 4: 1113, 5: 1109, 6: 1108, 7: 1110, 8: 1111, 9: 1116}, 'gap_year': 3475, 'industryphy_industryco': {1: 1211, 2: 1207, 3: 1188, 4: 1218, 5: 1217, 6: 1182, 7: 1204, 8: 1201, 9: 1183, 10: 1190, 11: 1177, 12: 1300, 13: 1178, 14: 1224, 15: 1249, 16: 1187, 17: 1203, 18: 1219, 19: 1226, 20: 1205, 21: 1214, 22: 1238, 23: 1228, 24: 1198, 25: 1234, 26: 1196, 27: 1415, 28: 1192, 29: 1197, 30: 1245, 31: 1262, 32: 1251, 33: 1222, 34: 1223, 35: 1210, 36: 1274, 37: 1346, 38: 1311, 39: 1299, 40: 1184, 41: 1186, 42: 1208, 43: 1199, 44: 1387, 45: 1195, 46: 1250, 47: 1193, 48: 1265, 49: 1272, 50: 1206, 51: 1212, 52: 1266, 53: 1209, 54: 1241, 55: 1247, 56: 1230, 57: 1441, 58: 1257, 59: 1330, 60: 1180, 61: 1295, 62: 1229, 63: 1185, 64: 1268, 65: 1236, 66: 1181, 67: 1281, 68: 1227, 69: 1320, 70: 1233, 71: 1248, 72: 1240, 73: 1261, 74: 1176, 75: 1316, 76: 1194, 77: 1255, 78: 1175, 79: 1302, 80: 1283, 81: 1270, 82: 1303, 83: 1334, 84: 1264, 85: 1246, 86: 1391, 87: 1215, 88: 1231, 89: 1353, 90: 1319, 91: 1273, 92: 1325, 93: 1239, 94: 1243, 95: 1278, 96: 1294, 97: 1436, 98: 1213, 99: 1442, 100: 1290, 101: 1235, 102: 1253, 103: 1254, 104: 1237, 105: 1354, 106: 1242, 107: 1332, 108: 1397, 109: 1244, 110: 1370, 111: 1324, 112: 1335, 113: 1438, 114: 1202, 115: 1340, 116: 1289, 117: 1263, 118: 1349, 119: 1341, 120: 1301, 121: 1309, 122: 1277, 123: 1305, 124: 1291, 125: 1439, 126: 1329, 127: 1318, 128: 1280, 129: 1225, 130: 1323, 131: 1258, 132: 1360, 133: 1220, 134: 1296, 135: 1389, 136: 1276, 137: 1348, 138: 1376, 139: 1339, 140: 1279, 141: 1400, 142: 1412, 143: 1307, 144: 1304, 145: 1361, 146: 1326, 147: 1331, 148: 1345, 149: 1423, 150: 1413, 151: 1232, 152: 1312, 153: 1416, 154: 1179, 155: 1189, 156: 1328, 157: 1372, 158: 1379, 159: 1347, 160: 1430, 161: 1336, 162: 1443, 163: 1333, 164: 1356, 165: 1435, 166: 1384, 167: 1252, 168: 1385, 169: 1315, 170: 1440, 171: 1448, 172: 1447, 173: 1350, 174: 1275, 175: 1297, 176: 1308, 177: 1259, 178: 1402, 179: 1369, 180: 1365, 181: 1418, 182: 1426, 183: 1191, 184: 1371, 185: 1433, 186: 1285, 187: 1420, 188: 1342, 189: 1355, 190: 1367, 191: 1322, 192: 1221, 193: 1374, 194: 1343, 195: 1306, 196: 1292, 197: 1394, 198: 1314, 199: 1375, 200: 1359, 201: 1256, 202: 1396, 203: 1368, 204: 1388, 205: 1405, 206: 1419, 207: 1390, 208: 1362, 209: 1288, 210: 1363, 211: 1429, 212: 1417, 213: 1267, 214: 1409, 215: 1431, 216: 1200, 217: 1282, 218: 1337, 219: 1344, 220: 1395, 221: 1410, 222: 1422, 223: 1401, 224: 1437, 225: 1407, 226: 1393, 227: 1380, 228: 1327, 229: 1364, 230: 1287, 231: 1392, 232: 1216, 233: 1313, 234: 1382, 235: 1399, 236: 1373, 237: 1381, 238: 1414, 239: 1383, 240: 1321, 241: 1386, 242: 1445, 243: 1434, 244: 1444, 245: 1358, 246: 1428, 247: 1357, 248: 1269, 249: 1403, 250: 1310, 251: 1378, 252: 1421, 253: 1298, 254: 1351, 255: 1411, 256: 1284, 257: 1286, 258: 1427, 259: 1408, 260: 1338, 261: 1398, 262: 1424, 263: 1260, 264: 1432, 265: 1366, 266: 1317, 267: 1271, 268: 1406, 269: 1404, 270: 1377, 271: 1446, 272: 1352, 273: 1293, 274: 1425}, 'enttypegb': {1: 366, 2: 362, 3: 363, 4: 367, 5: 365, 6: 373, 7: 369, 8: 364, 9: 370, 10: 371, 11: 384, 12: 378, 13: 380, 14: 386, 15: 376, 16: 393, 17: 392, 18: 379, 19: 382, 20: 397, 21: 394, 22: 387, 23: 396, 24: 375, 25: 402, 26: 403, 27: 381, 28: 372, 29: 404, 30: 390, 31: 400, 32: 406, 33: 374, 34: 385, 35: 398, 36: 383, 37: 377, 38: 391, 39: 368, 40: 388, 41: 405, 42: 399, 43: 401, 44: 395, 45: 389}, 'enttypeminu': {1: 340, 2: 342, 3: 339, 4: 341, 5: 345, 6: 348, 7: 344, 8: 352, 9: 346, 10: 353, 11: 349, 12: 354, 13: 359, 14: 360, 15: 357, 16: 343, 17: 347, 18: 355, 19: 351, 20: 350, 21: 361, 22: 356, 23: 358, -1: 338}, 'bgxmdm': 3479, 'industryphy': {1: 14, 2: 16, 3: 18, 4: 17, 5: 22, 6: 15, 7: 20, 8: 23, 9: 28, 10: 19, 11: 26, 12: 29, 13: 24, 14: 30, 15: 27, 16: 21, 17: 25, 18: 32, 19: 31}, 'exenum': 3472, 'FORINVESTSIGN': {1.0: 3399, -1.0: 3398}, 'industryco': {1: 69, 2: 65, 3: 46, 4: 76, 5: 75, 6: 40, 7: 62, 8: 59, 9: 41, 10: 48, 11: 35, 12: 158, 13: 36, 14: 82, 15: 107, 16: 45, 17: 61, 18: 77, 19: 84, 20: 63, 21: 72, 22: 96, 23: 86, 24: 56, 25: 92, 26: 54, 27: 273, 28: 50, 29: 55, 30: 103, 31: 120, 32: 109, 33: 80, 34: 81, 35: 68, 36: 132, 37: 204, 38: 169, 39: 157, 40: 42, 41: 44, 42: 66, 43: 57, 44: 245, 45: 53, 46: 108, 47: 51, 48: 123, 49: 130, 50: 64, 51: 70, 52: 124, 53: 67, 54: 99, 55: 105, 56: 88, 57: 299, 58: 115, 59: 188, 60: 38, 61: 153, 62: 87, 63: 43, 64: 126, 65: 94, 66: 39, 67: 139, 68: 85, 69: 178, 70: 91, 71: 106, 72: 98, 73: 119, 74: 34, 75: 174, 76: 52, 77: 113, 78: 33, 79: 160, 80: 141, 81: 128, 82: 161, 83: 192, 84: 122, 85: 104, 86: 249, 87: 73, 88: 89, 89: 211, 90: 177, 91: 131, 92: 183, 93: 97, 94: 101, 95: 136, 96: 152, 97: 294, 98: 71, 99: 300, 100: 148, 101: 93, 102: 111, 103: 112, 104: 95, 105: 212, 106: 100, 107: 190, 108: 255, 109: 102, 110: 228, 111: 182, 112: 193, 113: 296, 114: 60, 115: 198, 116: 147, 117: 121, 118: 207, 119: 199, 120: 159, 121: 167, 122: 135, 123: 163, 124: 149, 125: 297, 126: 187, 127: 176, 128: 138, 129: 83, 130: 181, 131: 116, 132: 218, 133: 78, 134: 154, 135: 247, 136: 134, 137: 206, 138: 234, 139: 197, 140: 137, 141: 258, 142: 270, 143: 165, 144: 162, 145: 219, 146: 184, 147: 189, 148: 203, 149: 281, 150: 271, 151: 90, 152: 170, 153: 274, 154: 37, 155: 47, 156: 186, 157: 230, 158: 237, 159: 205, 160: 288, 161: 194, 162: 301, 163: 191, 164: 214, 165: 293, 166: 242, 167: 110, 168: 243, 169: 173, 170: 298, 171: 306, 172: 305, 173: 208, 174: 133, 175: 155, 176: 166, 177: 117, 178: 260, 179: 227, 180: 223, 181: 276, 182: 284, 183: 49, 184: 229, 185: 291, 186: 143, 187: 278, 188: 200, 189: 213, 190: 225, 191: 180, 192: 79, 193: 232, 194: 201, 195: 164, 196: 150, 197: 252, 198: 172, 199: 233, 200: 217, 201: 114, 202: 254, 203: 226, 204: 246, 205: 263, 206: 277, 207: 248, 208: 220, 209: 146, 210: 221, 211: 287, 212: 275, 213: 125, 214: 267, 215: 289, 216: 58, 217: 140, 218: 195, 219: 202, 220: 253, 221: 268, 222: 280, 223: 259, 224: 295, 225: 265, 226: 251, 227: 238, 228: 185, 229: 222, 230: 145, 231: 250, 232: 74, 233: 171, 234: 240, 235: 257, 236: 231, 237: 239, 238: 272, 239: 241, 240: 179, 241: 244, 242: 303, 243: 292, 244: 302, 245: 216, 246: 286, 247: 215, 248: 127, 249: 261, 250: 168, 251: 236, 252: 279, 253: 156, 254: 209, 255: 269, 256: 142, 257: 144, 258: 285, 259: 266, 260: 196, 261: 256, 262: 282, 263: 118, 264: 290, 265: 224, 266: 175, 267: 129, 268: 264, 269: 262, 270: 235, 271: 304, 272: 210, 273: 151, -1: 283}, 'regcap_reccap': {1: 411, 2: 418, 3: 415, 4: 419, 5: 429, 6: 407, 7: 426, 8: 510, 9: 452, 10: 731, 11: 659, 12: 470, 13: 410, 14: 694, 15: 424, 16: 454, 17: 420, 18: 517, 19: 413, 20: 506, 21: 432, 22: 427, 23: 461, 24: 573, 25: 951, 26: 628, 27: 457, 28: 806, 29: 408, 30: 687, 31: 451, 32: 448, 33: 798, 34: 421, 35: 417, 36: 422, 37: 775, 38: 425, 39: 681, 40: 471, 41: 673, 42: 759, 43: 990, 44: 635, 45: 584, 46: 445, 47: 945, 48: 467, 49: 481, 50: 992, 51: 477, 52: 444, 53: 530, 54: 958, 55: 462, 56: 701, 57: 761, 58: 1037, 59: 552, 60: 869, 61: 827, 62: 1100, 63: 414, 64: 439, 65: 486, 66: 450, 67: 562, 68: 519, 69: 537, 70: 779, 71: 453, 72: 664, 73: 1102, 74: 493, 75: 1063, 76: 570, 77: 434, 78: 704, 79: 583, 80: 533, 81: 631, 82: 1101, 83: 505, 84: 483, 85: 1014, 86: 790, 87: 485, 88: 627, 89: 559, 90: 563, 91: 550, 92: 569, 93: 813, 94: 412, 95: 689, 96: 882, 97: 442, 98: 666, 99: 496, 100: 757, 101: 723, 102: 459, 103: 528, 104: 819, 105: 861, 106: 581, 107: 710, 108: 500, 109: 556, 110: 745, 111: 713, 112: 504, 113: 431, 114: 526, 115: 1027, 116: 613, 117: 863, 118: 1041, 119: 925, 120: 589, 121: 1080, 122: 1043, 123: 592, 124: 912, 125: 836, 126: 900, 127: 804, 128: 502, 129: 499, 130: 655, 131: 576, 132: 894, 133: 919, 134: 668, 135: 582, 136: 671, 137: 590, 138: 1016, 139: 579, 140: 661, 141: 548, 142: 1085, 143: 762, 144: 994, 145: 601, 146: 498, 147: 984, 148: 885, 149: 460, 150: 777, 151: 513, 152: 788, 153: 618, 154: 1015, 155: 565, 156: 793, 157: 587, 158: 624, 159: 512, 160: 647, 161: 547, 162: 503, 163: 826, 164: 972, 165: 743, 166: 588, 167: 1068, 168: 971, 169: 524, 170: 553, 171: 911, 172: 1092, 173: 698, 174: 580, 175: 699, 176: 960, 177: 538, 178: 974, 179: 717, 180: 943, 181: 438, 182: 455, 183: 938, 184: 539, 185: 492, 186: 730, 187: 621, 188: 932, 189: 648, 190: 910, 191: 1028, 192: 1029, 193: 725, 194: 516, 195: 1054, 196: 1030, 197: 416, 198: 615, 199: 609, 200: 479, 201: 711, 202: 598, 203: 472, 204: 829, 205: 1008, 206: 525, 207: 458, 208: 981, 209: 653, 210: 703, 211: 541, 212: 437, 213: 706, 214: 617, 215: 973, 216: 995, 217: 578, 218: 773, 219: 740, 220: 950, 221: 853, 222: 852, 223: 476, 224: 554, 225: 1035, 226: 854, 227: 1066, 228: 722, 229: 575, 230: 902, 231: 660, 232: 808, 233: 620, 234: 1081, 235: 1060, 236: 441, 237: 572, 238: 532, 239: 549, 240: 670, 241: 947, 242: 430, 243: 993, 244: 558, 245: 501, 246: 688, 247: 567, 248: 737, 249: 456, 250: 1064, 251: 866, 252: 491, 253: 1007, 254: 1070, 255: 1095, 256: 561, 257: 1088, 258: 769, 259: 634, 260: 443, 261: 930, 262: 625, 263: 656, 264: 639, 265: 729, 266: 800, 267: 1079, 268: 622, 269: 478, 270: 1067, 271: 997, 272: 633, 273: 856, 274: 864, 275: 828, 276: 809, 277: 887, 278: 832, 279: 566, 280: 893, 281: 600, 282: 447, 283: 969, 284: 497, 285: 679, 286: 878, 287: 658, 288: 529, 289: 663, 290: 1006, 291: 744, 292: 855, 293: 727, 294: 514, 295: 954, 296: 1018, 297: 1062, 298: 468, 299: 1076, 300: 606, 301: 629, 302: 520, 303: 1004, 304: 944, 305: 780, 306: 1073, 307: 544, 308: 1049, 309: 1084, 310: 748, 311: 812, 312: 840, 313: 815, 314: 446, 315: 1002, 316: 463, 317: 484, 318: 1036, 319: 669, 320: 1013, 321: 577, 322: 523, 323: 1051, 324: 867, 325: 645, 326: 916, 327: 746, 328: 754, 329: 604, 330: 895, 331: 1106, 332: 835, 333: 678, 334: 1055, 335: 654, 336: 929, 337: 435, 338: 758, 339: 789, 340: 1011, 341: 886, 342: 843, 343: 946, 344: 983, 345: 1074, 346: 899, 347: 735, 348: 1057, 349: 1005, 350: 630, 351: 1082, 352: 776, 353: 651, 354: 957, 355: 1098, 356: 488, 357: 1046, 358: 782, 359: 545, 360: 555, 361: 646, 362: 774, 363: 616, 364: 636, 365: 574, 366: 1023, 367: 814, 368: 507, 369: 883, 370: 738, 371: 822, 372: 862, 373: 585, 374: 714, 375: 920, 376: 968, 377: 469, 378: 1078, 379: 935, 380: 771, 381: 787, 382: 695, 383: 733, 384: 796, 385: 662, 386: 901, 387: 638, 388: 890, 389: 807, 390: 611, 391: 702, 392: 924, 393: 511, 394: 786, 395: 480, 396: 612, 397: 837, 398: 490, 399: 1091, 400: 436, 401: 888, 402: 1024, 403: 768, 404: 1050, 405: 847, 406: 850, 407: 657, 408: 898, 409: 881, 410: 939, 411: 1059, 412: 1032, 413: 641, 414: 752, 415: 1056, 416: 937, 417: 767, 418: 760, 419: 987, 420: 474, 421: 778, 422: 753, 423: 534, 424: 982, 425: 962, 426: 823, 427: 536, 428: 675, 429: 715, 430: 557, 431: 961, 432: 996, 433: 676, 434: 965, 435: 650, 436: 665, 437: 693, 438: 1022, 439: 1075, 440: 619, 441: 1021, 442: 527, 443: 440, 444: 1107, 445: 865, 446: 667, 447: 597, 448: 834, 449: 794, 450: 978, 451: 1090, 452: 831, 453: 874, 454: 423, 455: 747, 456: 1105, 457: 936, 458: 546, 459: 880, 460: 811, 461: 1104, 462: 966, 463: 792, 464: 720, 465: 998, 466: 551, 467: 677, 468: 724, 469: 610, 470: 728, 471: 591, 472: 842, 473: 964, 474: 1069, 475: 599, 476: 719, 477: 858, 478: 876, 479: 649, 480: 805, 481: 521, 482: 465, 483: 907, 484: 1033, 485: 489, 486: 891, 487: 923, 488: 917, 489: 914, 490: 991, 491: 637, 492: 1094, 493: 685, 494: 980, 495: 712, 496: 1034, 497: 640, 498: 1019, 499: 770, 500: 518, 501: 873, 502: 726, 503: 1087, 504: 801, 505: 707, 506: 892, 507: 1026, 508: 845, 509: 922, 510: 643, 511: 803, 512: 830, 513: 970, 514: 896, 515: 959, 516: 531, 517: 857, 518: 594, 519: 1065, 520: 953, 521: 877, 522: 475, 523: 1047, 524: 871, 525: 642, 526: 942, 527: 721, 528: 963, 529: 466, 530: 732, 531: 817, 532: 1048, 533: 608, 534: 568, 535: 602, 536: 652, 537: 846, 538: 906, 539: 1077, 540: 979, 541: 903, 542: 1001, 543: 897, 544: 756, 545: 464, 546: 875, 547: 956, 548: 734, 549: 449, 550: 931, 551: 690, 552: 1093, 553: 682, 554: 542, 555: 783, 556: 868, 557: 705, 558: 739, 559: 623, 560: 751, 561: 833, 562: 841, 563: 791, 564: 473, 565: 433, 566: 795, 567: 692, 568: 1052, 569: 1040, 570: 1010, 571: 428, 572: 976, 573: 742, 574: 696, 575: 1103, 576: 851, 577: 940, 578: 839, 579: 586, 580: 1099, 581: 948, 582: 595, 583: 967, 584: 1097, 585: 838, 586: 755, 587: 860, 588: 674, 589: 764, 590: 926, 591: 988, 592: 821, 593: 825, 594: 1089, 595: 844, 596: 750, 597: 913, 598: 908, 599: 820, 600: 684, 601: 785, 602: 697, 603: 799, 604: 1042, 605: 884, 606: 1053, 607: 802, 608: 1058, 609: 691, 610: 1003, 611: 508, 612: 540, 613: 605, 614: 909, 615: 781, 616: 1038, 617: 824, 618: 686, 619: 603, 620: 766, 621: 905, 622: 772, 623: 672, 624: 614, 625: 626, 626: 810, 627: 1096, 628: 933, 629: 989, 630: 708, 631: 632, 632: 918, 633: 700, 634: 509, 635: 955, 636: 607, 637: 564, 638: 535, 639: 1031, 640: 872, 641: 716, 642: 999, 643: 482, 644: 848, 645: 1045, 646: 921, 647: 596, 648: 644, 649: 1044, 650: 543, 651: 1071, 652: 763, 653: 1061, 654: 560, 655: 977, 656: 985, 657: 487, 658: 927, 659: 818, 660: 1009, 661: 879, 662: 409, 663: 1000, 664: 593, 665: 1086, 666: 522, 667: 941, 668: 741, 669: 904, 670: 928, 671: 736, 672: 934, 673: 870, 674: 975, 675: 1017, 676: 683, 677: 718, 678: 495, 679: 1012, 680: 709, 681: 797, 682: 986, 683: 1072, 684: 1025, 685: 849, 686: 494, 687: 952, 688: 859, 689: 1020, 690: 889, 691: 784, 692: 816, 693: 915, 694: 749, 695: 515, 696: 571, 697: 949, 698: 1083, 699: 765, 700: 1039, 701: 680}, 'TAX_AMOUNT': 3478, 'regcap': 3473}\n"
     ]
    }
   ],
   "source": [
    "# feature_dict = {}\n",
    "# total_feature = 0\n",
    "# for col in df.columns:\n",
    "#     if col in IGNORE_COLS:\n",
    "#         continue\n",
    "#     elif col in NUMERIC_COLS:\n",
    "#         feature_dict[col] = total_feature\n",
    "#         total_feature += 1\n",
    "#     else:\n",
    "#         unique_val = df[col].unique()\n",
    "#         feature_dict[col] = dict(zip(unique_val,range(total_feature,len(unique_val) + total_feature)))\n",
    "#         total_feature += len(unique_val)\n",
    "# print(total_feature)\n",
    "# print(feature_dict)\n",
    "feature_dict = {}\n",
    "total_feature = 0\n",
    "for col in df.columns:\n",
    "    if col in IGNORE_COLS:\n",
    "        continue\n",
    "    elif col in NUMERIC_COLS:\n",
    "        feature_dict[col] = total_feature\n",
    "        total_feature += 1\n",
    "    else:\n",
    "        unique_val = df[col].unique()\n",
    "        feature_dict[col] = dict(zip(unique_val,range(total_feature,len(unique_val) + total_feature)))\n",
    "        total_feature += len(unique_val)\n",
    "print(total_feature)\n",
    "print(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "F:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\"\"\"模型参数\"\"\"\n",
    "# params\n",
    "dfm_params = {\n",
    "    \"use_fm\": True,\n",
    "    \"use_deep\": True,\n",
    "    \"embedding_size\": 8,\n",
    "    \"dropout_fm\": [1.0, 1.0],\n",
    "    \"deep_layers\": [32, 32],\n",
    "    \"dropout_deep\": [0.5, 0.5, 0.5],\n",
    "    \"deep_layers_activation\": tf.nn.relu,\n",
    "    \"epoch\": 30,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer_type\": \"adam\",\n",
    "    \"batch_norm\": 1,\n",
    "    \"batch_norm_decay\": 0.995,\n",
    "    \"l2_reg\": 0.01,\n",
    "    \"verbose\": True,\n",
    "    \"eval_metric\": roc_auc_score,\n",
    "    \"random_seed\": 2017\n",
    "}\n",
    "dfm_params['feature_size'] = total_feature\n",
    "dfm_params['field_size'] = len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepFM import DeepFM\n",
    "# init a DeepFM model\n",
    "dfm = DeepFM(**dfm_params)\n",
    "\n",
    "# # fit a DeepFM model\n",
    "# dfm.fit(Xi, Xv, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(oob_score=True, random_state=2020,\n",
    "            n_estimators= 70,max_depth=13,min_samples_split=5,class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = ytrain\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,precision_recall_fscore_support,roc_curve,auc,roc_auc_score\n",
    "def eval_score(y_test,y_pre):\n",
    "    _,_,f_class,_=precision_recall_fscore_support(y_true=y_test,y_pred=y_pre,labels=[0,1],average=None)\n",
    "    fper_class={'合法':f_class[0],'违法':f_class[1],'f1':f1_score(y_test,y_pre)}\n",
    "    return fper_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机森林组合deepfm\n",
    "# mean_f1=0\n",
    "# mean_f1Train=0\n",
    "# n_splits=5\n",
    "# sk = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2020)\n",
    "# answers = []\n",
    "# for train, test in sk.split(dfTrain, kind):\n",
    "#     x_train = dfTrain.iloc[train]\n",
    "#     y_train = kind.iloc[train]\n",
    "#     x_test = dfTrain.iloc[test]\n",
    "#     y_test = kind.iloc[test]\n",
    "    \n",
    "#     #y_train = pd.get_dummies(y_train)\n",
    "#     #y_test = pd.get_dummies(y_test)\n",
    "\n",
    "#     \"\"\"\n",
    "#     对训练集进行转化\n",
    "#     \"\"\"\n",
    "#     y_train = np.array(y_train).reshape(-1,).tolist()\n",
    "#     #y_train = (np.array(y_train).reshape(-1,2)).astype(int).tolist()\n",
    "\n",
    "#     train_feature_index = x_train.copy()\n",
    "#     train_feature_value = x_train.copy()\n",
    "\n",
    "#     for col in train_feature_index.columns:\n",
    "#         if col in IGNORE_COLS:\n",
    "#             train_feature_index.drop(col,axis=1,inplace=True)\n",
    "#             train_feature_value.drop(col,axis=1,inplace=True)\n",
    "#             continue\n",
    "#         elif col in NUMERIC_COLS:\n",
    "#             train_feature_index[col] = feature_dict[col]\n",
    "#         else:\n",
    "#             train_feature_index[col] = train_feature_index[col].map(feature_dict[col])\n",
    "#             train_feature_value[col] = 1\n",
    "\n",
    "#     # list of list of feature indices of each sample in the dataset\n",
    "#     Xi = train_feature_index.values.tolist()\n",
    "#     # list of list of feature values of each sample in the dataset\n",
    "#     Xv = train_feature_value.values.tolist()\n",
    "\n",
    "#     \"\"\"\n",
    "#     对验证集进行转化\n",
    "#     \"\"\"\n",
    "#     y_test = np.array(y_test).reshape(-1,).tolist()\n",
    "#     test_feature_index = x_test.copy()\n",
    "#     test_feature_value = x_test.copy()\n",
    "\n",
    "#     for col in test_feature_index.columns:\n",
    "#         if col in IGNORE_COLS:\n",
    "#             test_feature_index.drop(col,axis=1,inplace=True)\n",
    "#             test_feature_value.drop(col,axis=1,inplace=True)\n",
    "#             continue\n",
    "#         elif col in NUMERIC_COLS:\n",
    "#             test_feature_index[col] = feature_dict[col]\n",
    "#         else:\n",
    "#             test_feature_index[col] = test_feature_index[col].map(feature_dict[col])\n",
    "#             test_feature_value[col] = 1\n",
    "#     Xi_valid = test_feature_index.values.tolist()\n",
    "#     Xv_valid = test_feature_value.values.tolist()\n",
    "\n",
    "#     \"\"\"\n",
    "#     对测试集进行转化\n",
    "#     \"\"\"\n",
    "\n",
    "#     test_feature_index1 = dfTest.copy()\n",
    "#     test_feature_value1 = dfTest.copy()\n",
    "#     test_feature_index1\n",
    "\n",
    "#     for col in df.columns:\n",
    "#         if col in IGNORE_COLS:\n",
    "#             test_feature_index1.drop(col,axis=1,inplace=True)\n",
    "#             test_feature_value1.drop(col,axis=1,inplace=True)\n",
    "#             continue\n",
    "#         elif col in NUMERIC_COLS:\n",
    "#             test_feature_index1[col] = feature_dict[col]\n",
    "#         else:\n",
    "#             test_feature_index1[col] = test_feature_index1[col].map(feature_dict[col])\n",
    "#             test_feature_value1[col] = 1\n",
    "#     # list of list of feature indices of each sample in the dataset\n",
    "#     Xi_test = test_feature_index1.values.tolist()\n",
    "#     Xv_test = test_feature_value1.values.tolist()\n",
    "\n",
    "#     dfm.fit(Xi, Xv, y_train, Xi_valid, Xv_valid, y_test, early_stopping=True)\n",
    "#     dfinput = pd.DataFrame(dfm.out_concat_input(Xi,Xv))\n",
    "#     dfinput_v = pd.DataFrame(dfm.out_concat_input(Xi_valid,Xv_valid))\n",
    "#     dfinput_t = pd.DataFrame(dfm.out_concat_input(Xi_test,Xv_test))\n",
    "# #     pred = dfm.predict(Xi_valid, Xv_valid)\n",
    "# #     fper_class =  eval_score(yvalid, pred)\n",
    "# #     mean_f1+=fper_class['f1']/n_splits\n",
    "#     #print(fper_class)\n",
    "#     #mean_dfinput += dfinput/n_splits\n",
    "#     rf.fit(dfinput, y_train)\n",
    "#     pred = rf.predict(dfinput_v)\n",
    "#     fper_class =  eval_score(y_test, pred)\n",
    "#     mean_f1+=fper_class['f1']/n_splits\n",
    "    \n",
    "#     test_rf = rf.predict_proba(dfinput_t)\n",
    "#     answers.append(test_rf)\n",
    "\n",
    "# fina=sum(answers)/n_splits#\n",
    "# #fina=np.sqrt(sum(np.array(answers)**2)/n_splits)#平方平均\n",
    "# #fina=fina[:,1]\n",
    "# #print('mean valf1:',mean_f1)\n",
    "# #print('mean trainf1:',mean_f1Train)\n",
    "# print(mean_f1)\n",
    "# print(fina.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.8045977011494253, '合法': 0.9876602951850955, '违法': 0.8045977011494253}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #单独使用deepfm结果\n",
    "# fina2 = dfm.predict(Xi_test,Xv_test)\n",
    "# m =[]\n",
    "# for i in fina2:\n",
    "#     if i>=0.5:\n",
    "#         m.append(1)\n",
    "#     else:\n",
    "#         m.append(0)\n",
    "# ytest\n",
    "# eval_score(ytest, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.8120300751879698, '合法': 0.9878875968992248, '违法': 0.8120300751879698}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #随机森林组合deepfm\n",
    "# m1 =[]\n",
    "# for i in fina[:,1]:\n",
    "#     if i>=0.5:\n",
    "#         m1.append(1)\n",
    "#     else:\n",
    "#         m1.append(0)\n",
    "# eval_score(ytest,m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7583699169200536\n",
      "(4394, 2)\n"
     ]
    }
   ],
   "source": [
    "# #单独使用随机森林\n",
    "# kind=ytrain\n",
    "# mean_f1=0\n",
    "# mean_f1Train=0\n",
    "# n_splits=5\n",
    "# sk1 = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2020)\n",
    "# answers = []\n",
    "# for train, test in sk1.split(dfTrain, kind):\n",
    "#     x_train = dfTrain.iloc[train]\n",
    "#     y_train = kind.iloc[train]\n",
    "#     x_test = dfTrain.iloc[test]\n",
    "#     y_test = kind.iloc[test]\n",
    "    \n",
    "#     rf.fit(x_train, y_train)\n",
    "#     pred = rf.predict(x_test)\n",
    "#     fper_class =  eval_score(y_test, pred)\n",
    "#     mean_f1+=fper_class['f1']/n_splits\n",
    "    \n",
    "#     test_rf = rf.predict_proba(dfTest)\n",
    "#     answers.append(test_rf)\n",
    "\n",
    "# fina=sum(answers)/n_splits#\n",
    "# #fina=np.sqrt(sum(np.array(answers)**2)/n_splits)#平方平均\n",
    "# #fina=fina[:,1]\n",
    "# #print('mean valf1:',mean_f1)\n",
    "# #print('mean trainf1:',mean_f1Train)\n",
    "# print(mean_f1)\n",
    "# print(fina.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.7887323943661972, '合法': 0.9854014598540147, '违法': 0.7887323943661972}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #单独使用随机森林\n",
    "# m2 =[]\n",
    "# for i in fina[:,1]:\n",
    "#     if i>=0.5:\n",
    "#         m2.append(1)\n",
    "#     else:\n",
    "#         m2.append(0)\n",
    "# eval_score(ytest,m2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 14.0]\n"
     ]
    }
   ],
   "source": [
    "#使用树模型融合\n",
    "xlf=xgb.XGBClassifier(max_depth=7\n",
    "                      ,learning_rate=0.05\n",
    "                      ,n_estimators=53\n",
    "                      ,reg_alpha=0.005\n",
    "                      ,n_jobs=8\n",
    "                      ,importance_type='total_cover'\n",
    "                     ,is_unbalance=True)\n",
    "\n",
    "llf=lgb.LGBMClassifier(num_leaves=9\n",
    "                           ,max_depth=5\n",
    "                           ,learning_rate=0.05\n",
    "                           ,n_estimators=80\n",
    "                           ,n_jobs=8\n",
    "                           ,is_unbalance=True)\n",
    "class_weights = np.array([1., 14.])\n",
    "class_weights = class_weights.tolist()\n",
    "print(class_weights)\n",
    "clf=cab.CatBoostClassifier(iterations=60\n",
    "                             ,learning_rate=0.05\n",
    "                             ,depth=10\n",
    "                             ,silent=True\n",
    "                             ,thread_count=8\n",
    "                             ,task_type='CPU'\n",
    "                             ,cat_features=None\n",
    "                             ,class_weights=class_weights)\n",
    "\n",
    "rf = RandomForestClassifier(oob_score=True, random_state=2020,\n",
    "            n_estimators= 70,max_depth=13,min_samples_split=5,class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:22:24] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { is_unbalance } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "每1次验证的f1:0.7906976744186047\n",
      "[12:22:30] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { is_unbalance } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "每2次验证的f1:0.8016528925619835\n",
      "[12:22:35] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { is_unbalance } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "每3次验证的f1:0.8101265822784811\n",
      "[12:22:41] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { is_unbalance } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "每4次验证的f1:0.7929515418502202\n",
      "[12:22:47] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { is_unbalance } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "每5次验证的f1:0.8148148148148148\n",
      "mean f1: 0.8020487011848209\n"
     ]
    }
   ],
   "source": [
    "#模型融合，最终方法\n",
    "details = []\n",
    "answers = []\n",
    "mean_f1=0\n",
    "n_splits=5\n",
    "sk = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2020)\n",
    "cnt=0\n",
    "for train, test in sk.split(dfTrain, kind):\n",
    "    x_train = dfTrain.iloc[train]\n",
    "    y_train = kind.iloc[train]\n",
    "    x_test = dfTrain.iloc[test]\n",
    "    y_test = kind.iloc[test]\n",
    "\n",
    "    xlf.fit(x_train, y_train)\n",
    "    pred_xgb = xlf.predict(x_test)\n",
    "    weight_xgb = eval_score(y_test,pred_xgb)['f1']\n",
    "\n",
    "    llf.fit(x_train, y_train)\n",
    "    pred_llf = llf.predict(x_test)\n",
    "    weight_lgb = eval_score(y_test,pred_llf)['f1']\n",
    "\n",
    "    clf.fit(x_train, y_train)\n",
    "    pred_cab = clf.predict(x_test)\n",
    "    weight_cab =  eval_score(y_test,pred_cab)['f1']\n",
    "\n",
    "    rf.fit(x_train, y_train)\n",
    "    pred_rf = rf.predict(x_test)\n",
    "    weight_rf =  eval_score(y_test,pred_rf)['f1']\n",
    "\n",
    "\n",
    "    prob_xgb = xlf.predict_proba(x_test)\n",
    "    prob_lgb = llf.predict_proba(x_test)\n",
    "    prob_cab = clf.predict_proba(x_test)\n",
    "    prob_rf = rf.predict_proba(x_test)\n",
    "\n",
    "    scores = []\n",
    "    ijkl = []\n",
    "    weight = np.arange(0, 1.05, 0.1)\n",
    "    for i, item1 in enumerate(weight):\n",
    "        for j, item2 in enumerate(weight[weight <= (1 - item1)]):\n",
    "            for k, item3 in enumerate(weight[weight <= (1 - item1-item2)]):\n",
    "                prob_end = prob_xgb * item1 + prob_lgb * item2 + prob_cab *item3+prob_rf*(1 - item1 - item2-item3)\n",
    "                #prob_end = np.sqrt(prob_xgb**2 * item1 + prob_lgb**2 * item2 + prob_cab**2 *item3+prob_rf**2*(1 - item1 - item2-item3))\n",
    "                score = eval_score(y_test,np.argmax(prob_end,axis=1))['f1']\n",
    "                scores.append(score)\n",
    "                ijkl.append((item1, item2,item3, 1 - item1 - item2-item3))\n",
    "\n",
    "    ii = ijkl[np.argmax(scores)][0]\n",
    "    jj = ijkl[np.argmax(scores)][1]\n",
    "    kk = ijkl[np.argmax(scores)][2]\n",
    "    ll = ijkl[np.argmax(scores)][3]\n",
    "\n",
    "    details.append(max(scores))\n",
    "    details.append(weight_xgb)\n",
    "    details.append(weight_lgb)\n",
    "    details.append(weight_cab)\n",
    "    details.append(weight_rf)\n",
    "    details.append(ii)\n",
    "    details.append(jj)\n",
    "    details.append(kk)\n",
    "    details.append(ll)\n",
    "\n",
    "    cnt+=1\n",
    "    print('每{}次验证的f1:{}'.format(cnt,max(scores)))\n",
    "    mean_f1+=max(scores)/n_splits\n",
    "\n",
    "    test_xgb = xlf.predict_proba(dfTest)\n",
    "    test_lgb = llf.predict_proba(dfTest)\n",
    "    test_cab = clf.predict_proba(dfTest)\n",
    "    test_rf = rf.predict_proba(dfTest)\n",
    "    #加权平均\n",
    "    ans = test_xgb * ii + test_lgb * jj + test_cab * kk + test_rf*ll#加权平均\n",
    "    #加权平方平均\n",
    "    #ans = np.sqrt(test_xgb**2 * ii + test_lgb**2 * jj + test_cab**2 * kk + test_rf**2*ll)\n",
    "    answers.append(ans)\n",
    "print('mean f1:',mean_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.8451242829827916, '合法': 0.9901996370235935, '违法': 0.8451242829827916}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#模型融合在测试集上的效果\n",
    "fina=sum(answers)/n_splits\n",
    "m3 =[]\n",
    "for i in fina[:,1]:\n",
    "    if i>=0.5:\n",
    "        m3.append(1)\n",
    "    else:\n",
    "        m3.append(0)\n",
    "eval_score(ytest,m3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] train-result=0.9972, valid-result=0.9981 [0.3 s]\n",
      "[2] train-result=0.9975, valid-result=0.9979 [0.3 s]\n",
      "[3] train-result=0.9973, valid-result=0.9979 [0.3 s]\n",
      "[4] train-result=0.9974, valid-result=0.9978 [0.3 s]\n",
      "[5] train-result=0.9973, valid-result=0.9977 [0.3 s]\n",
      "[12:23:15] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { is_unbalance } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "每6次验证的f1:0.8879999999999999\n",
      "[1] train-result=0.9964, valid-result=0.9980 [0.5 s]\n",
      "[2] train-result=0.9971, valid-result=0.9980 [0.4 s]\n",
      "[3] train-result=0.9962, valid-result=0.9981 [0.5 s]\n",
      "[4] train-result=0.9967, valid-result=0.9980 [0.4 s]\n",
      "[5] train-result=0.9971, valid-result=0.9978 [0.4 s]\n",
      "[6] train-result=0.9966, valid-result=0.9977 [0.5 s]\n",
      "[7] train-result=0.9971, valid-result=0.9975 [0.5 s]\n",
      "[12:23:37] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { is_unbalance } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "每7次验证的f1:0.9029535864978903\n",
      "[1] train-result=0.9973, valid-result=0.9986 [0.3 s]\n",
      "[2] train-result=0.9967, valid-result=0.9984 [0.3 s]\n",
      "[3] train-result=0.9972, valid-result=0.9984 [0.3 s]\n",
      "[4] train-result=0.9971, valid-result=0.9983 [0.3 s]\n",
      "[5] train-result=0.9966, valid-result=0.9981 [0.3 s]\n",
      "[12:23:55] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { is_unbalance } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "每8次验证的f1:0.894308943089431\n",
      "[1] train-result=0.9967, valid-result=0.9976 [0.4 s]\n",
      "[12:24:14] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { is_unbalance } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "每9次验证的f1:0.9180327868852458\n",
      "[1] train-result=0.9981, valid-result=0.9942 [0.3 s]\n",
      "[12:24:31] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { is_unbalance } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "每10次验证的f1:0.9401709401709402\n",
      "mean f1: 0.9086932513287014\n"
     ]
    }
   ],
   "source": [
    "#使用deepfm与树模型融合\n",
    "mean_f1=0\n",
    "mean_f1Train=0\n",
    "n_splits=5\n",
    "sk = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2020)\n",
    "answers = []\n",
    "for train, test in sk.split(dfTrain, kind):\n",
    "    x_train = dfTrain.iloc[train]\n",
    "    y_train = kind.iloc[train]\n",
    "    x_test = dfTrain.iloc[test]\n",
    "    y_test = kind.iloc[test]\n",
    "    \n",
    "    #y_train = pd.get_dummies(y_train)\n",
    "    #y_test = pd.get_dummies(y_test)\n",
    "\n",
    "    \"\"\"\n",
    "    对训练集进行转化\n",
    "    \"\"\"\n",
    "    y_train = np.array(y_train).reshape(-1,).tolist()\n",
    "    #y_train = (np.array(y_train).reshape(-1,2)).astype(int).tolist()\n",
    "\n",
    "    train_feature_index = x_train.copy()\n",
    "    train_feature_value = x_train.copy()\n",
    "\n",
    "    for col in train_feature_index.columns:\n",
    "        if col in IGNORE_COLS:\n",
    "            train_feature_index.drop(col,axis=1,inplace=True)\n",
    "            train_feature_value.drop(col,axis=1,inplace=True)\n",
    "            continue\n",
    "        elif col in NUMERIC_COLS:\n",
    "            train_feature_index[col] = feature_dict[col]\n",
    "        else:\n",
    "            train_feature_index[col] = train_feature_index[col].map(feature_dict[col])\n",
    "            train_feature_value[col] = 1\n",
    "\n",
    "    # list of list of feature indices of each sample in the dataset\n",
    "    Xi = train_feature_index.values.tolist()\n",
    "    # list of list of feature values of each sample in the dataset\n",
    "    Xv = train_feature_value.values.tolist()\n",
    "\n",
    "    \"\"\"\n",
    "    对验证集进行转化\n",
    "    \"\"\"\n",
    "    y_test = np.array(y_test).reshape(-1,).tolist()\n",
    "    test_feature_index = x_test.copy()\n",
    "    test_feature_value = x_test.copy()\n",
    "\n",
    "    for col in test_feature_index.columns:\n",
    "        if col in IGNORE_COLS:\n",
    "            test_feature_index.drop(col,axis=1,inplace=True)\n",
    "            test_feature_value.drop(col,axis=1,inplace=True)\n",
    "            continue\n",
    "        elif col in NUMERIC_COLS:\n",
    "            test_feature_index[col] = feature_dict[col]\n",
    "        else:\n",
    "            test_feature_index[col] = test_feature_index[col].map(feature_dict[col])\n",
    "            test_feature_value[col] = 1\n",
    "    Xi_valid = test_feature_index.values.tolist()\n",
    "    Xv_valid = test_feature_value.values.tolist()\n",
    "\n",
    "    \"\"\"\n",
    "    对测试集进行转化\n",
    "    \"\"\"\n",
    "\n",
    "    test_feature_index1 = dfTest.copy()\n",
    "    test_feature_value1 = dfTest.copy()\n",
    "    test_feature_index1\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in IGNORE_COLS:\n",
    "            test_feature_index1.drop(col,axis=1,inplace=True)\n",
    "            test_feature_value1.drop(col,axis=1,inplace=True)\n",
    "            continue\n",
    "        elif col in NUMERIC_COLS:\n",
    "            test_feature_index1[col] = feature_dict[col]\n",
    "        else:\n",
    "            test_feature_index1[col] = test_feature_index1[col].map(feature_dict[col])\n",
    "            test_feature_value1[col] = 1\n",
    "    # list of list of feature indices of each sample in the dataset\n",
    "    Xi_test = test_feature_index1.values.tolist()\n",
    "    Xv_test = test_feature_value1.values.tolist()\n",
    "\n",
    "    dfm.fit(Xi, Xv, y_train, Xi_valid, Xv_valid, y_test, early_stopping=True)\n",
    "    dfinput = pd.DataFrame(dfm.out_concat_input(Xi,Xv))\n",
    "    dfinput_v = pd.DataFrame(dfm.out_concat_input(Xi_valid,Xv_valid))\n",
    "    dfinput_t = pd.DataFrame(dfm.out_concat_input(Xi_test,Xv_test))\n",
    "#     pred = dfm.predict(Xi_valid, Xv_valid)\n",
    "#     fper_class =  eval_score(yvalid, pred)\n",
    "#     mean_f1+=fper_class['f1']/n_splits\n",
    "    #print(fper_class)\n",
    "    #mean_dfinput += dfinput/n_splits\n",
    "    xlf.fit(dfinput, y_train)\n",
    "    pred_xgb = xlf.predict(dfinput_v)\n",
    "    weight_xgb = eval_score(y_test,pred_xgb)['f1']\n",
    "\n",
    "    llf.fit(dfinput, y_train)\n",
    "    pred_llf = llf.predict(dfinput_v)\n",
    "    weight_lgb = eval_score(y_test,pred_llf)['f1']\n",
    "\n",
    "    clf.fit(dfinput, y_train)\n",
    "    pred_cab = clf.predict(dfinput_v)\n",
    "    weight_cab =  eval_score(y_test,pred_cab)['f1']\n",
    "\n",
    "    rf.fit(dfinput, y_train)\n",
    "    pred_rf = rf.predict(dfinput_v)\n",
    "    weight_rf =  eval_score(y_test,pred_rf)['f1']\n",
    "\n",
    "\n",
    "    prob_xgb = xlf.predict_proba(dfinput_v)\n",
    "    prob_lgb = llf.predict_proba(dfinput_v)\n",
    "    prob_cab = clf.predict_proba(dfinput_v)\n",
    "    prob_rf = rf.predict_proba(dfinput_v)\n",
    "\n",
    "    scores = []\n",
    "    ijkl = []\n",
    "    weight = np.arange(0, 1.05, 0.1)\n",
    "    for i, item1 in enumerate(weight):\n",
    "        for j, item2 in enumerate(weight[weight <= (1 - item1)]):\n",
    "            for k, item3 in enumerate(weight[weight <= (1 - item1-item2)]):\n",
    "                prob_end = prob_xgb * item1 + prob_lgb * item2 + prob_cab *item3+prob_rf*(1 - item1 - item2-item3)\n",
    "                #prob_end = np.sqrt(prob_xgb**2 * item1 + prob_lgb**2 * item2 + prob_cab**2 *item3+prob_rf**2*(1 - item1 - item2-item3))\n",
    "                score = eval_score(y_test,np.argmax(prob_end,axis=1))['f1']\n",
    "                scores.append(score)\n",
    "                ijkl.append((item1, item2,item3, 1 - item1 - item2-item3))\n",
    "\n",
    "    ii = ijkl[np.argmax(scores)][0]\n",
    "    jj = ijkl[np.argmax(scores)][1]\n",
    "    kk = ijkl[np.argmax(scores)][2]\n",
    "    ll = ijkl[np.argmax(scores)][3]\n",
    "\n",
    "    details.append(max(scores))\n",
    "    details.append(weight_xgb)\n",
    "    details.append(weight_lgb)\n",
    "    details.append(weight_cab)\n",
    "    details.append(weight_rf)\n",
    "    details.append(ii)\n",
    "    details.append(jj)\n",
    "    details.append(kk)\n",
    "    details.append(ll)\n",
    "\n",
    "    cnt+=1\n",
    "    print('每{}次验证的f1:{}'.format(cnt,max(scores)))\n",
    "    mean_f1+=max(scores)/n_splits\n",
    "\n",
    "    test_xgb = xlf.predict_proba(dfinput_t)\n",
    "    test_lgb = llf.predict_proba(dfinput_t)\n",
    "    test_cab = clf.predict_proba(dfinput_t)\n",
    "    test_rf = rf.predict_proba(dfinput_t)\n",
    "    #加权平均\n",
    "    ans = test_xgb * ii + test_lgb * jj + test_cab * kk + test_rf*ll#加权平均\n",
    "    #加权平方平均\n",
    "    #ans = np.sqrt(test_xgb**2 * ii + test_lgb**2 * jj + test_cab**2 * kk + test_rf**2*ll)\n",
    "    answers.append(ans)\n",
    "print('mean f1:',mean_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.7984344422700587, '合法': 0.9875558777334783, '违法': 0.7984344422700587}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#单独使用deepfm结果\n",
    "fina2 = dfm.predict(Xi_test,Xv_test)\n",
    "m =[]\n",
    "for i in fina2:\n",
    "    if i>=0.5:\n",
    "        m.append(1)\n",
    "    else:\n",
    "        m.append(0)\n",
    "ytest\n",
    "eval_score(ytest, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.7939508506616257, '合法': 0.9868022763046373, '违法': 0.7939508506616257}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用deepfm与树模型融合组合的结果，验证集高，但测试集低\n",
    "fina=sum(answers)/n_splits\n",
    "m4 =[]\n",
    "for i in fina[:,1]:\n",
    "    if i>=0.5:\n",
    "        m4.append(1)\n",
    "    else:\n",
    "        m4.append(0)\n",
    "eval_score(ytest,m4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({\"id\": Test1['id'], \"score\": y_pred.flatten()}).to_csv(\n",
    "#         \"data/pred_result.csv\", index=False, float_format=\"%.5f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
