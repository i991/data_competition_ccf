{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "#from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cab\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,precision_recall_fscore_support,roc_curve,auc,roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#from featexp import get_univariate_plots#用于特征筛选，需要先安装featexp\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['font.sans-serif']=['Simhei']\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "import json\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext\n",
    "#import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 数据的简单分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_info shape: (24865, 33) id unique: 24865\n",
      "annual_report_info shape: (22550, 23) id unique: 8937\n",
      "tax_info shape: (29195, 9) id unique: 808\n",
      "change_info shape: (45940, 5) id unique: 8726\n",
      "news_info shape: (10518, 3) id unique: 927\n",
      "other_info shape: (1890, 4) id unique: 1888\n",
      "entprise_info shape: (14865, 2) id unique: 14865\n",
      "entprise_evaluate shape: (10000, 2) id unique: 10000\n"
     ]
    }
   ],
   "source": [
    "base_info=pd.read_csv('train/base_info.csv')#企业的基本信息\n",
    "annual_report_info=pd.read_csv('train/annual_report_info.csv')#企业的年报基本信息\n",
    "tax_info=pd.read_csv('train/tax_info.csv')#企业的纳税信息\n",
    "change_info=pd.read_csv('train/change_info.csv')#变更信息\n",
    "news_info=pd.read_csv('train/news_info.csv')#舆情信息\n",
    "other_info=pd.read_csv('train/other_info.csv')#其它信息\n",
    "entprise_info=pd.read_csv('train/entprise_info.csv')#企业标注信息{0: 13884, 1: 981}\n",
    "entprise_evaluate=pd.read_csv('entprise_evaluate.csv')#未标注信息\n",
    "\n",
    "print('base_info shape:',base_info.shape,'id unique:',len(base_info['id'].unique()))\n",
    "print('annual_report_info shape:',annual_report_info.shape,'id unique:',len(annual_report_info['id'].unique()))\n",
    "print('tax_info shape:',tax_info.shape,'id unique:',len(tax_info['id'].unique()))\n",
    "print('change_info shape:',change_info.shape,'id unique:',len(change_info['id'].unique()))\n",
    "print('news_info shape:',news_info.shape,'id unique:',len(news_info['id'].unique()))\n",
    "print('other_info shape:',other_info.shape,'id unique:',len(other_info['id'].unique()))\n",
    "print('entprise_info shape:',entprise_info.shape,'id unique:',len(entprise_info['id'].unique()))\n",
    "print('entprise_evaluate shape:',entprise_evaluate.shape,'id unique:',len(entprise_evaluate['id'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 特征构建 \n",
    "###  tfidi处理经营范围(opscope)特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.290 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对opscope提取tfidif特征完毕..........\n"
     ]
    }
   ],
   "source": [
    "# tfidif 处理经营范围的特征\n",
    "#cn_stopwords.txt来源于 https://github.com/goto456/stopwords\n",
    "def stopwordslist():\n",
    "    stopwords = [line.strip() for line in open('stopwords-master/cn_stopwords.txt',encoding='UTF-8').readlines()]\n",
    "    return stopwords\n",
    "# 创建一个停用词列表\n",
    "stopwords = stopwordslist()\n",
    "stopwords+=['、', '；', '，', '）','（']\n",
    "#\n",
    "train_df_scope=base_info.merge(entprise_info)[['id','opscope','label']]\n",
    "test_df_scope=base_info[base_info['id'].isin(entprise_evaluate['id'].unique().tolist())]\n",
    "test_df_scope=test_df_scope.reset_index(drop=True)[['id','opscope']]\n",
    "str_label_0=''\n",
    "str_label_1=''\n",
    "for index,name,opscope,label in train_df_scope.itertuples():\n",
    "    # 结巴分词\n",
    "    seg_text = jieba.cut(opscope.replace(\"\\t\", \" \").replace(\"\\n\", \" \"))\n",
    "    outline = \" \".join(seg_text)\n",
    "    out_str=\"\"\n",
    "    for per in outline.split():\n",
    "        if per not in stopwords: \n",
    "            out_str += per\n",
    "            out_str+=\" \"\n",
    "    if label==0:\n",
    "        str_label_0+=out_str\n",
    "    else:\n",
    "        str_label_1+=out_str\n",
    "corpus=[str_label_0,str_label_1]\n",
    "vectorizer=CountVectorizer()#该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "transformer=TfidfTransformer()#该类会统计每个词语的tf-idf权值\n",
    "tfidf=transformer.fit_transform(vectorizer.fit_transform(corpus))#第一个fit_transform是计算tf-idf，第二个fit_transform是将文本转为词频矩阵\n",
    "word=vectorizer.get_feature_names()#获取词袋模型中的所有词语总共7175个词语\n",
    "weight=tfidf.toarray()#将(2, 7175)tf-idf矩阵抽取出来，元素a[i][j]表示j词在i类文本中的tf-idf权重\n",
    "# for i in range(len(weight)):#打印每类文本的tf-idf词语权重，第一个for遍历所有文本，第二个for便利某一类文本下的词语权重\n",
    "#     #\n",
    "#     for j in range(len(word)):\n",
    "#         print(word[j],weight[i][j])\n",
    "#下面将会根据tfidi算出来的权重将经营范围的文本特征转换为数值(利用weight[1,:]也即各个词语在第二类(违法类中所占据的权重之和))\n",
    "illegal_word_weights={}\n",
    "for i in range(len(word)):\n",
    "    illegal_word_weights[word[i]]=weight[1][i]\n",
    "tfidi_opscope=[]\n",
    "for index,name,opscope in base_info[['id','opscope']].itertuples():\n",
    "    # \n",
    "    seg_text = jieba.cut(opscope.replace(\"\\t\", \" \").replace(\"\\n\", \" \"))\n",
    "    outline = \" \".join(seg_text)\n",
    "    tfidi_frt=0\n",
    "    for per in outline.split():\n",
    "        if per in illegal_word_weights: \n",
    "            tfidi_frt+=illegal_word_weights[per]\n",
    "    tfidi_opscope.append(tfidi_frt)\n",
    "base_info['tfidif_opscope']=tfidi_opscope\n",
    "print('对opscope提取tfidif特征完毕..........')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  change_info、other_info，news_info，annual_report_info,tax表格的简单特征构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished .............\n"
     ]
    }
   ],
   "source": [
    "#change_info\n",
    "change_info_clean=change_info.drop(['bgrq','bgq','bgh'],axis=1)\n",
    "change_info_clean = change_info_clean.groupby('id',sort=False).agg('mean')\n",
    "change_info_clean=pd.DataFrame(change_info_clean).reset_index()\n",
    "#other_info\n",
    "#空值大于0.5的列都删除掉\n",
    "buf_group = other_info.groupby('id',sort=False).agg('mean')\n",
    "other_info_clean=pd.DataFrame(buf_group).reset_index()\n",
    "other_info_clean=other_info_clean.fillna(-1)\n",
    "other_info_clean = other_info_clean.groupby('id',sort=False).agg('mean')\n",
    "other_info_clean=pd.DataFrame(other_info_clean).reset_index()\n",
    "#news_info\n",
    "news_info_clean=news_info.drop(['public_date'],axis=1)\n",
    "#对object类型进行编码\n",
    "news_info_clean['positive_negtive']=news_info_clean['positive_negtive'].fillna(\"中立\")\n",
    "#\n",
    "dic={}\n",
    "cate=news_info_clean.positive_negtive.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "#\n",
    "news_info_clean['positive_negtive']=news_info_clean['positive_negtive'].map(dic)\n",
    "news_info_clean = news_info_clean.groupby('id',sort=False).agg('mean')\n",
    "news_info_clean=pd.DataFrame(news_info_clean).reset_index()\n",
    "#处理annual_report_info的数据\n",
    "#空值大于0.5的列都删除掉\n",
    "annual_report_info_clean=annual_report_info.dropna(thresh=annual_report_info.shape[0]*0.5,how='all',axis=1)\n",
    "#对object类型进行编码\n",
    "annual_report_info_clean['BUSSTNAME']=annual_report_info_clean['BUSSTNAME'].fillna(\"无\")\n",
    "dic = {'无':-1,'开业':0, '歇业':1, '停业':2, '清算':3}\n",
    "#\n",
    "annual_report_info_clean['BUSSTNAME']=annual_report_info_clean['BUSSTNAME'].map(dic)\n",
    "annual_report_info_clean = annual_report_info_clean.groupby('id',sort=False).agg('mean')\n",
    "annual_report_info_clean=pd.DataFrame(annual_report_info_clean).reset_index()\n",
    "#处理tax数据,构建数值特征\n",
    "tax_info_clean=tax_info.copy()\n",
    "tax_info_clean['START_DATE']=pd.to_datetime(tax_info_clean['START_DATE'])\n",
    "tax_info_clean['END_DATE']=pd.to_datetime(tax_info_clean['END_DATE'])\n",
    "tax_info_clean['gap_day']=(tax_info_clean['END_DATE']-tax_info_clean['START_DATE']).dt.total_seconds()//3600//24\n",
    "tax_info_clean=tax_info_clean.drop(['START_DATE','END_DATE'],axis=1)\n",
    "tax_info_clean['TAX_CATEGORIES']=tax_info_clean['TAX_CATEGORIES'].fillna(\"无\")#17 unique\n",
    "tax_info_clean['TAX_ITEMS']=tax_info_clean['TAX_ITEMS'].fillna(\"无\")#275 TAX_ITEMS\n",
    "#对object类型进行编码\n",
    "dic={}\n",
    "cate=tax_info_clean.TAX_CATEGORIES.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "tax_info_clean['TAX_CATEGORIES']=tax_info_clean['TAX_CATEGORIES'].map(dic)\n",
    "#\n",
    "dic={}\n",
    "cate=tax_info_clean.TAX_ITEMS.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "tax_info_clean['TAX_ITEMS']=tax_info_clean['TAX_ITEMS'].map(dic)\n",
    "tax_info_clean['income']=tax_info_clean['TAX_AMOUNT']/tax_info_clean['TAX_RATE']\n",
    "#\n",
    "tax_info_clean = tax_info_clean.groupby('id',sort=False).agg('mean')\n",
    "tax_info_clean=pd.DataFrame(tax_info_clean).reset_index()\n",
    "#税额分箱\n",
    "tax_info_clean['TAX_AMOUNT']=tax_info_clean['TAX_AMOUNT'].fillna(tax_info_clean['TAX_AMOUNT'].median())\n",
    "tax_info_clean['bucket_TAX_AMOUNT']=pd.qcut(tax_info_clean['TAX_AMOUNT'], 10, labels=False,duplicates='drop')\n",
    "print('finished .............')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## base_info数据较为重要，需要构建诸多交叉特征以及特征分箱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                | 0/24865 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码完毕.................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 24865/24865 [00:01<00:00, 24520.29it/s]\n",
      "100%|█████████████████████████████████| 24865/24865 [00:00<00:00, 25926.57it/s]\n",
      "100%|█████████████████████████████████| 24865/24865 [00:00<00:00, 25980.75it/s]\n",
      "100%|█████████████████████████████████| 24865/24865 [00:00<00:00, 26089.80it/s]\n",
      "100%|█████████████████████████████████| 24865/24865 [00:00<00:00, 28480.61it/s]\n",
      "100%|█████████████████████████████████| 24865/24865 [00:01<00:00, 24568.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24865, 42)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #处理base_info数据\n",
    "base_info['opto']=pd.to_datetime(base_info['opto']).fillna(pd.to_datetime(base_info['opto']).max())\n",
    "base_info['opfrom']=pd.to_datetime(base_info['opfrom'])\n",
    "base_info['gap_year']=(base_info['opto']-base_info['opfrom']).dt.total_seconds()//3600//24//365\n",
    "base_info_clean=base_info.drop(['opscope','opfrom','opto'],axis=1)\n",
    "\n",
    "#............................对object类型进行编码...............................\n",
    "base_info_clean['industryphy']=base_info_clean['industryphy'].fillna(\"无\")\n",
    "base_info_clean['dom']=base_info_clean['dom'].fillna(\"无\")\n",
    "base_info_clean['opform']=base_info_clean['opform'].fillna(\"无\")\n",
    "base_info_clean['oploc']=base_info_clean['oploc'].fillna(\"无\")\n",
    "#\n",
    "dic={}\n",
    "cate=base_info_clean.industryphy.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "base_info_clean['industryphy']=base_info_clean['industryphy'].map(dic)\n",
    "#\n",
    "dic={}\n",
    "cate=base_info_clean.dom.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "base_info_clean['dom']=base_info_clean['dom'].map(dic)\n",
    "#\n",
    "dic={}\n",
    "cate=base_info_clean.opform.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "base_info_clean['opform']=base_info_clean['opform'].map(dic)\n",
    "#\n",
    "dic={}\n",
    "cate=base_info_clean.oploc.unique()\n",
    "for i in range(len(cate)):\n",
    "    dic[cate[i]]=i\n",
    "base_info_clean['oploc']=base_info_clean['oploc'].map(dic)\n",
    "#\n",
    "base_info_clean=base_info_clean.fillna(-1)\n",
    "#\n",
    "print('编码完毕.................')\n",
    "#........................分箱.................................\n",
    "#在哪用了?\n",
    "def bucket(name,bucket_len):\n",
    "    gap_list=[base_info_clean[name].quantile(i/bucket_len) for i in range(bucket_len+1)]#以分位数作为分箱标志\n",
    "    len_data=len(base_info_clean[name])\n",
    "    new_col=[]\n",
    "    for i in base_info_clean[name].values:\n",
    "        for j in range(len(gap_list)):\n",
    "            if gap_list[j]>=i:\n",
    "                encode=j\n",
    "                break\n",
    "        new_col.append(encode)\n",
    "    return new_col\n",
    "#注册资本_实缴资本\n",
    "base_info_clean['regcap_reccap']=base_info_clean['regcap']-base_info_clean['reccap']\n",
    "#注册资本分箱\n",
    "base_info_clean['regcap']=base_info_clean['regcap'].fillna(base_info_clean['regcap'].median())\n",
    "base_info_clean['bucket_regcap']=pd.qcut(base_info_clean['regcap'], 10, labels=False,duplicates='drop')\n",
    "#实缴资本分箱\n",
    "base_info_clean['reccap']=base_info_clean['reccap'].fillna(base_info_clean['reccap'].median())\n",
    "base_info_clean['bucket_reccap']=pd.qcut(base_info_clean['reccap'], 10, labels=False,duplicates='drop')\n",
    "#注册资本_实缴资本分箱\n",
    "base_info_clean['regcap_reccap']=base_info_clean['regcap_reccap'].fillna(base_info_clean['regcap_reccap'].median())\n",
    "base_info_clean['bucket_regcap_reccap']=pd.qcut(base_info_clean['regcap_reccap'], 10, labels=False,duplicates='drop')\n",
    "#.............................交叉.........................\n",
    "#作两个特征的交叉\n",
    "def cross_two(name_1,name_2):\n",
    "    new_col=[]\n",
    "    encode=0\n",
    "    dic={}\n",
    "    val_1=base_info_clean[name_1]\n",
    "    val_2=base_info_clean[name_2]\n",
    "    for i in tqdm(range(len(val_1))):\n",
    "        tmp=str(val_1[i])+'_'+str(val_2[i])\n",
    "        if tmp in dic:\n",
    "            new_col.append(dic[tmp])\n",
    "        else:\n",
    "            dic[tmp]=encode\n",
    "            new_col.append(encode)\n",
    "            encode+=1\n",
    "    return new_col\n",
    "#作企业类型-小类的交叉特征\n",
    "base_info_clean['enttypegb']=base_info_clean['enttypegb'].fillna(\"无\")\n",
    "base_info_clean['enttypeitem']=base_info_clean['enttypeitem'].fillna(\"无\")\n",
    "new_col=cross_two('enttypegb','enttypeitem')#作企业类型-小类的交叉特征\n",
    "base_info_clean['enttypegb_enttypeitem']=new_col\n",
    "#\n",
    "#行业类别-细类的交叉特征\n",
    "base_info_clean['industryphy']=base_info_clean['industryphy'].fillna(\"无\")\n",
    "base_info_clean['industryco']=base_info_clean['industryco'].fillna(\"无\")\n",
    "new_col=cross_two('industryphy','industryco')#作企业类型-小类的交叉特征\n",
    "base_info_clean['industryphy_industryco']=new_col\n",
    "#企业类型-行业类别的交叉特征\n",
    "new_col=cross_two('enttypegb','industryphy')#作企业类型-小类的交叉特征\n",
    "base_info_clean['enttypegb_industryphy']=new_col\n",
    "#行业类别-企业类型小类的交叉特征\n",
    "new_col=cross_two('industryphy','enttypeitem')#作企业类型-小类的交叉特征\n",
    "base_info_clean['industryphy_enttypeitem']=new_col\n",
    "#行业类别细类--企业类型小类的交叉特征\n",
    "new_col=cross_two('industryco','enttypeitem')#作企业类型-小类的交叉特征\n",
    "base_info_clean['industryco_enttypeitem']=new_col\n",
    "\n",
    "#企业类型-小类-行业类别-细类的交叉特征\n",
    "new_col=cross_two('enttypegb_enttypeitem','industryphy_industryco')#作企业类型-小类的交叉特征\n",
    "base_info_clean['enttypegb_enttypeitem_industryphy_industryco']=new_col\n",
    "base_info_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24865, 73)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "all_data=base_info_clean.merge(annual_report_info_clean,how='outer')\n",
    "all_data=all_data.merge(tax_info_clean,how='outer')\n",
    "all_data=all_data.merge(change_info_clean,how='outer')\n",
    "all_data=all_data.merge(news_info_clean,how='outer')\n",
    "all_data=all_data.merge(other_info_clean,how='outer')\n",
    "all_data=all_data.fillna(-1)\n",
    "all_data[cat_features]=all_data[cat_features].astype(int)\n",
    "all_data.shape#,base_info.shape,annual_report_info.shape,tax_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "train_df=all_data.merge(entprise_info)\n",
    "train_data=train_df.drop(['id','label'],axis=1)\n",
    "kind=train_df['label']\n",
    "test_df=all_data[all_data['id'].isin(entprise_evaluate['id'].unique().tolist())]\n",
    "test_df=test_df.reset_index(drop=True)\n",
    "test_data=test_df.drop(['id'],axis=1)\n",
    "train_data.shape,test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_df.csv')\n",
    "test_df.to_csv('test_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3.6.5tensorflow)",
   "language": "python",
   "name": "py3.6.5tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
